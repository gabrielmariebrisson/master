{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse, classification et indexation des données: feuille 8\n",
    "### Réduction de dimension - Analyse linéaire discriminante (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avant de commencer\n",
    "\n",
    "Dans cet exercice, on s'intéresse à l'application de l'analyse linéaire discriminante (LDA) à un corpus de données. Le but est de réduire la dimension tout en conservant une séparation entre les classes. \n",
    "\n",
    "<i>Questions préliminaires : </i> \n",
    "\n",
    "1. Quelle est la différence entre une ACP et une LDA ?\n",
    "\n",
    "2. Quelle est la fonction objective que l'on cherche à minimiser quand on fait une LDA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1.\n",
    "\n",
    "Dans cet exercice, on considère le corpus sur la classification des vins. Ce corpus fait partie de l'ensemble des <code>datasets</code> du module <code>sklearn.datasets</code>. Il peut être chargé en invoquant la méthode <code>load_wine()</code>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données \n",
    "\n",
    "1. Chargez le corpus, explorez-le. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "import pandas as pa\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Créez un <code>DataFrame data</code>  contenant les variables indépendantes et mettez les classes dans une variable dépendante $target$. Quelle est la taille du corpus ? Combien de variables comporte-t-il ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "data = pa.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "target = pa.DataFrame(data=wine.target, columns=['class'])\n",
    "print(data.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "print(wine.feature_names)\n",
    "print(wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En utilisant la fonction <code>concat</code>, constituez un seul corpus <code>dataset</code> contenant et <code>data</code> et <code>target</code>. Faites un mélange des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "dataset = pa.concat([data, target], axis=1)\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA \"à la main\"\n",
    "\n",
    "Dans un premier temps, nous allons appliquer la méthode d'analyse linéaire discriminante juste sur deux classes. Nous allons donc d'abord ne garder que les vins des classes 1 et 2. Nous allons également ne conserver que deux descripteurs <code>alcohol</code> et <code>color_intensity</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ecrivez les instructions permettant de faire ce filtrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "dataset = dataset[['alcohol', 'color_intensity', 'class']][ dataset['class']!= 0]\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Quelles sont les étapes (théoriques) pour réaliser une LDA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORRECTION\n",
    "\n",
    " 1. On calcule les matrices de dispersion de chacune des classes S1 et S2\n",
    "\n",
    " 2. On calcule la matrice de dispersion intra-classes : \n",
    "\n",
    "$$\n",
    "S_w = S_1 + S_2.\n",
    "$$\n",
    "\n",
    "$$\n",
    "S_i = (n_i - 1) \\times COV_i\n",
    "$$\n",
    "    où $n_i$ est le nombre d'éléments de classe $i$ et $COV_i$ la matrice de covariance des variables restreintes à la classe $i$.  \n",
    "\n",
    " 3. Le vecteur de la meilleur direction est donné par :\n",
    "\n",
    "$$\n",
    "S_w^{-1} * (\\mu_1 - \\mu_2)\n",
    "$$\n",
    "    où $\\mu_i$ est la moyenne des variables restreintes à la classe $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Application numérique : appliquer les étapes de la questions précédente pour réaliser une LDA. Quel est le vecteur directeur du meilleur axe de projection selon la méthode LDA ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "y = dataset['class']\n",
    "X = dataset.drop(['class'], axis=1)\n",
    "\n",
    "def CalculLDA(X):\n",
    "    X1 = X[y==1]\n",
    "    X2 = X[y==2]\n",
    "    S1 = (len(X1)-1)*X1.cov()\n",
    "    S2 = (len(X2)-1)*X2.cov()\n",
    "    #print(S1)\n",
    "    #print(S2)\n",
    "    Sw = S1 + S2\n",
    "    #print(Sw)\n",
    "    invSw = inv(Sw)\n",
    "    mu1 = X[y==1].mean()\n",
    "    mu2 = X[y==2].mean()\n",
    "    return np.matmul(invSw, mu1 - mu2)\n",
    "    \n",
    "CalculLDA(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualiser le résultat. Donnez les deux graphiques, avant et après la projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "v = CalculLDA(X)\n",
    "X_proj = np.dot(X, v) #équivalent à X_proj = np.dot(v.T, X.T)\n",
    "print(X_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Avant la projection : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.scatter(X['alcohol'], X['color_intensity'], c=y)\n",
    "#print(Xsave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Après la projection :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "plt.scatter(X_proj, np.zeros(X_proj.shape[0]), c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA avec <code>sklearn</code>\n",
    "\n",
    "Réalisez une LDA en utilisant la bibliothèque <code>sklearn.discriminant_analysis</code>. Comparez le résultat avec  la projection que vous avez obtenu dans la section précédente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "wine = load_wine()\n",
    "X = pa.DataFrame(data=wine.data, columns=wine.feature_names)[['alcohol','color_intensity']][wine.target != 0]\n",
    "y = wine.target[wine.target != 0]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "plt.scatter(X_lda[:,0], np.zeros(X_lda.shape[0]), c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA sur tout le corpus \n",
    "\n",
    "Nous allons à présent appliquer la LDA sur tout le corpus. L'objectif est d'observer l'impact de la projection à la fois sur l'efficacité (<code>accuracy</code>) de la classification et sur le temps d'apprentissage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rechargez les données et faites votre LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "wine = load_wine()\n",
    "X = pa.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "ldaw = LinearDiscriminantAnalysis()\n",
    "X_ldaw = ldaw.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Quel est le ratio de la variance expliqué par les axes obtenus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "ldaw.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualiser le résultat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "plt.scatter(X_ldaw[:, 0], X_ldaw[:, 1], c=y)\n",
    "plt.xlabel('LDA1')\n",
    "plt.ylabel('LDA2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification avant/après réduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En utilisant un classifieur bayésien MAP, comparez les résultats obtenus avec les échantillons bruts et les résultats obtenus avec les échantillons projetés sur les axes fournis par la LDA. Comparez également les temps d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "wine = load_wine()\n",
    "X = pa.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb = GaussianNB()\n",
    "print('Sans réduction :')\n",
    "%timeit nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "#Avec réduction de dimension :\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "nb = GaussianNB()\n",
    "print('Après réduction :')\n",
    "%timeit nb.fit(X_train_lda, y_train)\n",
    "y_pred = nb.predict(X_test_lda)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Même question avec un $k$-nn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print('Sans réduction :')\n",
    "knn = KNeighborsClassifier()\n",
    "%timeit knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)\n",
    "\n",
    "#Avec réduction de dimension :\n",
    "print('Après réduction :')\n",
    "knn = KNeighborsClassifier()\n",
    "%timeit knn.fit(X_train_lda, y_train)\n",
    "y_pred = knn.predict(X_test_lda)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.\n",
    "\n",
    "Dans cet exercice, nous allons travailler avec le même corpus que le précédent TD : les données sur le cancer du sein. Le corpus peut être chargé par l'instruction <code> load_breast_cancer</code> de la bibliothèque <code>sklearn.datasets</code>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer les effets de l'ACP et de la LDA sur la classification de ce corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Faites une ACP en utilisant le module <code>PCA</code> de la bibliothèque <code>sklearn.decomposition</code>.  \n",
    "\n",
    "Attention : pensez à centrer et réduire vos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "z = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "acp = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "coord = acp.fit_transform(z)\n",
    "print(acp.n_components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Affichez l'éboulie des valeurs propres et indiquer le nombre d'axes à retenir en utilisant le critère du coude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.grid()\n",
    "plt.plot(np.arange(1,acp.n_components_+1),acp.explained_variance_ratio_) \n",
    "plt.title(\"Scree plot\") \n",
    "plt.ylabel(\"Eigen values\") \n",
    "plt.xlabel(\"Factor number\") \n",
    "plt.show()\n",
    "#On applique le critère du coude et on choisit de ne retenir que les trois premiers axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "\n",
    "p = 3\n",
    "acp = PCA(n_components=p)\n",
    "coord = acp.fit_transform(z)\n",
    "print(acp.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Voyons ce qu'est l'impact de l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "nb = GaussianNB()\n",
    "print('Sans réduction :')\n",
    "%timeit nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "print('Après réduction ACP:')\n",
    "ztrain = scaler.fit_transform(X_train)\n",
    "ztest = scaler.transform(X_test)\n",
    "acp = PCA(n_components=3)\n",
    "Xtrain_p = acp.fit_transform(ztrain)\n",
    "Xtest_p = acp.transform(ztest)\n",
    "ztrain = scaler.fit_transform(X_train)\n",
    "%timeit nb.fit(Xtrain_p, y_train)\n",
    "y_pred = nb.predict(Xtest_p)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. puis celui de la LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORRECTION\n",
    "print('Après réduction LDA:')\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "Xtrain_p = lda.fit_transform(X_train, y_train)\n",
    "Xtest_p = lda.transform(X_test)\n",
    "nb = GaussianNB()\n",
    "%timeit nb.fit(Xtrain_p, y_train)\n",
    "y_pred = nb.predict(Xtest_p)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ',acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
