{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda022d9-b5f3-42cf-84de-9543ba21c25f",
   "metadata": {},
   "source": [
    "#### Université de Bordeaux,  Master Mention Informatique\n",
    "\n",
    "# Analyse, classification et indexation des données: feuille 2\n",
    "### Introduction : métriques d'évaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5eda1-7056-4ae3-b27c-3fb5aec67bba",
   "metadata": {},
   "source": [
    "### Présentation\n",
    "\n",
    "L'objectif de ce TD est de comprendre les métriques d'évaluation des modèles de machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373820f8-2064-4d75-81cc-5c31fda8e339",
   "metadata": {},
   "source": [
    "### Exercice 1.\n",
    "On dispose d'un corpus de données $D = (x_i, y_i)_{1\\leq i \\leq n}$. On partitionne alors $D$ en deux ensembles :\n",
    "$$\n",
    "D = Train \\cup Test\n",
    "$$\n",
    "Le sous ensemble $Train$ est alors utilisé pour entrainer un modèle $M$ alors que $Test$ est lui utilisé pour tester $M$\n",
    "\n",
    "1. Rappeler la définition de la matrice de confusion \n",
    "2. On se limite au cas d'une classification binaire. Donner, en fonction du contenu de la matrice de confusion les expressions de l'$accuracy$, la $precision$, le $recall$ et le $F_1 score$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad64dad0",
   "metadata": {},
   "source": [
    "1. def: une matrice de confusion permet d'identitifié la validité des réponses d'un modele par rapport a la realité.\n",
    "\n",
    "2.  M[0,0]= true positif ; M[1,1]= true negatif ; M[1,0]= false negatif ; M[0,1]= false positif.\n",
    "\n",
    "    accuracy = (M[0,0]+M[1,1])/(M[0,0]+M[0,1]+M[1,0]+M[1,1]).$$\n",
    "    precision=  M[0,0]/(M[0,0]+M[0,1]).$$\n",
    "    recall/sensibily = M[0,0]/(M[0,0]+M[1,0]).$$\n",
    "    F1-score= M[0,0]/ (M[0,0] + (M[1,0]+M[0,1])/2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e11df6-ac02-4a5d-8155-ce2dee2ddfc3",
   "metadata": {},
   "source": [
    "### Exercice 2.\n",
    "\n",
    "On dispose de données (images IRM) relatives à des personnes atteintes (ou pas) de cancer. On entraîne alors un modèle pour détecter d'éventuelles cellules cancereuses sur les images. Celles-ci sont alors classifiées $1$ pour les images positives (patient atteint de cancer) et $0$ pour les images négatives.\n",
    "\n",
    "On a ensuite testé le modèle sur des données d'autres patients pour lesquels le diagnostic est connu. Le résultat peut être récupéré à l'adresse \n",
    "\n",
    "<code>https://www.labri.fr/perso/zemmari/datasets/y_test_pred.csv</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6454ce9-8f4f-4ac9-946d-2edb06d3436a",
   "metadata": {},
   "source": [
    "#### Question 1.\n",
    "\n",
    "Ecrire le code nécessaire pour charger le contenu du fichier dans un <code>DataFrame y</code>. On peut récupérer les premières lignes de <code> y </code> avec <code>y.head()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7b7bd9-d5b8-48f5-bf5f-c3b6b1720161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     vt  vp\n",
      "0     1   1\n",
      "1     0   1\n",
      "2     0   1\n",
      "3     1   1\n",
      "4     1   1\n",
      "..   ..  ..\n",
      "508   0   1\n",
      "509   1   1\n",
      "510   1   1\n",
      "511   0   1\n",
      "512   0   1\n",
      "\n",
      "[513 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv(\"https://www.labri.fr/perso/zemmari/datasets/y_test_pred.csv\")\n",
    "print(data)\n",
    "#y=data.head()\n",
    "y=data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd037cdf",
   "metadata": {},
   "source": [
    "Tester le code suivant pour apprendre comment accéder à une colonne d'un DataFrame et la transformer en numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd2772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "508    0\n",
       "509    1\n",
       "510    1\n",
       "511    0\n",
       "512    0\n",
       "Name: vt, Length: 513, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_vt = y[\"vt\"]\n",
    "y_vp = y[\"vp\"]\n",
    "y_vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed5be039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_vt= np.array(y_vt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8c4f6-9a1e-443d-b9e8-33b1afc77529",
   "metadata": {},
   "source": [
    "#### Question 2.\n",
    "Ecrire une fonction <code>compute_confusion_matrix(y_vt, y_vp)</code> permettant de calculer la matrice de confusion entre la vérité terrain <code>y_vt</code> et les prédictions <code>y_vp</code> (données dans le format pandas). La matrice de confusion calculée sera un numpy array 2x2.\n",
    "\n",
    "*Indication :*  la fonction <code>logical_and(exp1, exp2)</code> du package <code>numpy</code> retourne <code>1</code> si les deux expressions sont vraies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b63f6265-dc0f-4e13-94df-b6bc63b19b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_vt, y_vp):\n",
    "    MatriceConfusion=np.zeros((2, 2))\n",
    "    MatriceConfusion[0,0]= np.sum(np.logical_and(y_vt==0, y_vp==0))\n",
    "    MatriceConfusion[0,1]= np.sum(np.logical_and(y_vt==0, y_vp==1))\n",
    "    MatriceConfusion[1,0]= np.sum(np.logical_and(y_vt==1, y_vp==0))\n",
    "    MatriceConfusion[1,1]= np.sum(np.logical_and(y_vt==1, y_vp==1))\n",
    "    return MatriceConfusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf429ccb-19fe-4e9f-b720-f4bd91d5b4e9",
   "metadata": {},
   "source": [
    "Calculer la matrice de confusions des données ci-dessus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b35f4331-6835-4508-b96b-a4ef5455fb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M[ 0 , 0 ]= 3.0\n",
      "M[ 0 , 1 ]= 190.0\n",
      "M[ 1 , 0 ]= 3.0\n",
      "M[ 1 , 1 ]= 317.0\n"
     ]
    }
   ],
   "source": [
    "newMatrice=compute_confusion_matrix(y_vt, y_vp)\n",
    "\n",
    "def printMatrice(M):\n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            print(\"M[\",i,\",\",j,\"]=\",M[i,j])\n",
    "            \n",
    "printMatrice(newMatrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c54ba4d-0f83-4e7e-beb1-f7c466ce5a1b",
   "metadata": {},
   "source": [
    "#### Question 3.\n",
    "Ecrire une fonction <code>compute_metrics(confusion_matrix)</code> permettant de calculer $accuracy$, la $precision$, le $recall$ et le $F_1 score$ à partir de la matrice de confusion donnée sous la forme d'un numpy array 2x2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88ea4284-9e47-4b77-a26a-42811a1fb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(M):\n",
    "    TN=M[0,0]\n",
    "    FP=M[0,1]\n",
    "    FN=M[1,0]\n",
    "    TP=M[1,1]\n",
    "    accuracy = (TP+TN)/(TP+FN+FP+TN)\n",
    "    precision=  TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    F1_score= TP/ (TP + (FP+FN)/2)\n",
    "    return accuracy,precision,recall,F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c15a7-091a-4998-bdc6-f12e39479969",
   "metadata": {},
   "source": [
    "Utiliser la fonction pour évaluer le modèle. Qu'en pensez-vous ? Donner une interprétation de chacune des métriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48e976e4-44de-4249-9308-365c886dec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision: 0.6252465483234714\n",
      "Recall: 0.990625\n",
      "Accuracy: 0.6237816764132553\n",
      "F1-score: 0.03015075376884422\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, F1_score = compute_metrics(newMatrice)\n",
    "\n",
    "print(\"Précision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc01f3c",
   "metadata": {},
   "source": [
    "Précision (Precision) : La précision mesure la proportion des prédictions positives correctes parmi toutes les prédictions positives. Une précision de 0.625 signifie que 62,5 % des prédictions positives du modèle étaient correctes.\n",
    "\n",
    "Rappel (Recall) : Le rappel mesure la proportion des vraies positives correctement prédites parmi toutes les vraies positives réelles. Un rappel de 0.990625 signifie que le modèle a correctement identifié 99,06 % des cas réellement positifs.\n",
    "\n",
    "Exactitude (Accuracy) : L'exactitude mesure la proportion de toutes les prédictions correctes parmi toutes les prédictions effectuées par le modèle. Une exactitude de 0.6238 signifie que le modèle a correctement classé environ 62,38 % de toutes les instances.\n",
    "\n",
    "F1-Score : Le score F1 est une mesure qui combine à la fois la précision et le rappel en une seule métrique. Un score F1 de 0.0302 indique que le modèle atteint un équilibre relativement faible entre la précision et le rappel.\n",
    "\n",
    "\n",
    "En résumé, les résultats montrent que votre modèle a un bon rappel, ce qui signifie qu'il est capable de capturer la plupart des vraies positives, mais il a une précision relativement faible, ce qui signifie qu'il fait également beaucoup de fausses prédictions positives. L'exactitude est également faible, ce qui peut indiquer un déséquilibre de classe ou d'autres problèmes dans les données ou le modèle. Le score F1 est faible, suggérant que l'équilibre entre précision et rappel n'est pas optimal pour votre cas d'utilisation. Vous devrez peut-être ajuster votre modèle ou vos données pour améliorer ces métriques en fonction de vos objectifs spécifiques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf930b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
