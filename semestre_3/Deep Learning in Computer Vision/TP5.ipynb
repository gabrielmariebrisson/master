{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9ecdc34",
   "metadata": {},
   "source": [
    "# Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e289d75f-0191-4ce1-b92d-c715d7dcc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "345a82fe-5aad-4907-8b62-5ec79e1cd798",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        # Get the last convolutional layer\n",
    "        self.target_layer = model.layer4[-1].conv3\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register hooks\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients = grad_output[0]\n",
    "            \n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "        \n",
    "    def generate_cam(self, input_image, target_class=None):\n",
    "        # Forward pass\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        # Target for backprop\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0][target_class] = 1\n",
    "        \n",
    "        # Backward pass\n",
    "        output.backward(gradient=one_hot)\n",
    "        \n",
    "        # Get weights\n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[2, 3])\n",
    "        \n",
    "        # Weight the channels by corresponding gradients\n",
    "        for i in range(self.activations.shape[1]):\n",
    "            self.activations[:, i, :, :] *= pooled_gradients[0, i]\n",
    "            \n",
    "        # Generate heatmap\n",
    "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
    "        heatmap = F.relu(heatmap)  # ReLU to only keep positive influence\n",
    "        heatmap = heatmap / torch.max(heatmap)  # Normalize\n",
    "        \n",
    "        return heatmap.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4376538-796e-41ca-8041-1365c021927d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05b20d3d-19c9-447b-838b-cd637bd07498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    return input_tensor, image\n",
    "\n",
    "def visualize_cam(image_path, model, layer_name):\n",
    "    # Load and preprocess image\n",
    "    input_tensor, original_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Initialize GradCAM\n",
    "    grad_cam = GradCAM(model,layer_name)\n",
    "    \n",
    "    # Generate heatmap\n",
    "    heatmap = grad_cam.generate_cam(input_tensor)\n",
    "    \n",
    "    # Resize heatmap to match original image\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = Image.fromarray(heatmap).resize(\n",
    "        (original_image.size[0], original_image.size[1]),\n",
    "        Image.Resampling.BILINEAR\n",
    "    )\n",
    "    \n",
    "    # Convert to RGB for overlay\n",
    "    heatmap = np.array(heatmap)\n",
    "    heatmap = np.uint8(plt.cm.jet(heatmap)[..., :3] * 255)\n",
    "    \n",
    "    # Overlay heatmap on original image\n",
    "    original_array = np.array(original_image)\n",
    "    overlaid = 0.6 * original_array + 0.4 * heatmap\n",
    "    overlaid = np.clip(overlaid, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_array)\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(heatmap)\n",
    "    plt.title('Grad-CAM Heatmap')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlaid)\n",
    "    plt.title('Overlay')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52812f34-246e-4f34-a193-f27f56ecff63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subject4/LIME/data/African_elephant/ILSVRC2012_val_00048781.JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subject4/LIME/data/African_elephant/ILSVRC2012_val_00039678.JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./subject4/LIME/data/black_bear/ILSVRC2012_val_00014576.JPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[0;32m---> 13\u001b[0m     visualize_cam(image_path, model, layer_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv5_block3_out\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m other_model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvgg16(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m image_paths:\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mvisualize_cam\u001b[0;34m(image_path, model, layer_name)\u001b[0m\n\u001b[1;32m     15\u001b[0m input_tensor, original_image \u001b[38;5;241m=\u001b[39m preprocess_image(image_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize GradCAM\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m grad_cam \u001b[38;5;241m=\u001b[39m GradCAM(model,layer_name)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Generate heatmap\u001b[39;00m\n\u001b[1;32m     21\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m grad_cam\u001b[38;5;241m.\u001b[39mgenerate_cam(input_tensor)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mGradCAM.__init__\u001b[0;34m(self, model, layer_name)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_name \u001b[38;5;241m=\u001b[39m layer_name\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(\n\u001b[1;32m     13\u001b[0m     [model\u001b[38;5;241m.\u001b[39minputs],\n\u001b[1;32m     14\u001b[0m     [model\u001b[38;5;241m.\u001b[39mget_layer(layer_name)\u001b[38;5;241m.\u001b[39moutput, model\u001b[38;5;241m.\u001b[39moutput]\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Load pre-trained ResNet50 with latest weights\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define image paths\n",
    "image_paths = [\n",
    "    \"./subject4/LIME/data/African_elephant/ILSVRC2012_val_00048781.JPEG\",\n",
    "    \"./subject4/LIME/data/African_elephant/ILSVRC2012_val_00039678.JPEG\",\n",
    "    \"./subject4/LIME/data/black_bear/ILSVRC2012_val_00014576.JPEG\"\n",
    "]\n",
    "\n",
    "for image_path in image_paths:\n",
    "    visualize_cam(image_path, model, layer_name='conv5_block3_out')\n",
    "\n",
    "other_model = model = models.vgg16(pretrained=True)\n",
    "\n",
    "for image_path in image_paths:\n",
    "    visualize_cam(image_path, other_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152594a-7f71-4928-a41b-d59760bbe505",
   "metadata": {},
   "source": [
    "# Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950429b-2cb9-496f-8c09-24194f1aeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FEM:\n",
    "    def __init__(self, model, k_sigma=2):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.k_sigma = k_sigma\n",
    "        # Get the last convolutional layer\n",
    "        self.target_layer = model.layer4[-1].conv3\n",
    "        self.activations = None\n",
    "        \n",
    "        # Register forward hook\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "            \n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        \n",
    "    def generate_heatmap(self, input_image):\n",
    "        # Forward pass to get feature maps\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_image)\n",
    "            predicted_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        # Get feature maps from last conv layer\n",
    "        feature_maps = self.activations.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Calculate mean and std for each feature map\n",
    "        means = []\n",
    "        stds = []\n",
    "        thresholded_maps = []\n",
    "        \n",
    "        # Process each feature map\n",
    "        for feat_map in feature_maps:\n",
    "            feat_map_np = feat_map.cpu().numpy()\n",
    "            mean = np.mean(feat_map_np)\n",
    "            std = np.std(feat_map_np)\n",
    "            \n",
    "            # Apply K-sigma thresholding\n",
    "            threshold = mean + (self.k_sigma * std)\n",
    "            thresholded = np.where(feat_map_np > threshold, feat_map_np, 0)\n",
    "            thresholded_maps.append(thresholded)\n",
    "            \n",
    "            means.append(mean)\n",
    "            stds.append(std)\n",
    "        \n",
    "        # Stack thresholded maps\n",
    "        all_thresholded = np.stack(thresholded_maps)\n",
    "        \n",
    "        # Combine feature maps\n",
    "        combined_map = np.sum(all_thresholded, axis=0)\n",
    "        \n",
    "        # Normalize\n",
    "        if combined_map.max() != 0:\n",
    "            combined_map = (combined_map - combined_map.min()) / (combined_map.max() - combined_map.min())\n",
    "            \n",
    "        return combined_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09851fdc-4624-439e-a3c7-fab2fac6067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fem(image_path, model, k_sigma=2):\n",
    "    # Load and preprocess image\n",
    "    input_tensor, original_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Initialize FEM\n",
    "    fem = FEM(model, k_sigma=k_sigma)\n",
    "    \n",
    "    # Generate heatmap\n",
    "    heatmap, predicted_class = fem.generate_heatmap(input_tensor)\n",
    "    \n",
    "    # Get class name\n",
    "    with open('imagenet_classes.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    predicted_class_name = classes[predicted_class]\n",
    "    \n",
    "    # Resize heatmap to match original image\n",
    "    heatmap_resized = Image.fromarray(np.uint8(255 * heatmap)).resize(\n",
    "        (original_image.size[0], original_image.size[1]),\n",
    "        Image.Resampling.BILINEAR\n",
    "    )\n",
    "    \n",
    "    # Convert to RGB for overlay\n",
    "    heatmap_colored = np.uint8(plt.cm.jet(np.array(heatmap_resized))[..., :3] * 255)\n",
    "    \n",
    "    # Create overlay\n",
    "    original_array = np.array(original_image)\n",
    "    overlaid = 0.7 * original_array + 0.3 * heatmap_colored\n",
    "    overlaid = np.clip(overlaid, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(original_array)\n",
    "    plt.title(f'Original Image\\nPredicted: {predicted_class_name}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(heatmap_colored)\n",
    "    plt.title(f'FEM Heatmap\\n(k={k_sigma}Ïƒ)')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(overlaid)\n",
    "    plt.title('Overlay')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return heatmap, predicted_class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8a7ce-5664-4c8d-898b-88e6582e0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_fem_gradcam(image_path, model):\n",
    "    from gradcam import GradCAM  # Import from previous implementation\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    input_tensor, original_image = preprocess_image(image_path)\n",
    "    \n",
    "    # Generate FEM heatmap\n",
    "    fem = FEM(model)\n",
    "    fem_heatmap, predicted_class = fem.generate_heatmap(input_tensor)\n",
    "    \n",
    "    # Generate Grad-CAM heatmap\n",
    "    grad_cam = GradCAM(model)\n",
    "    grad_cam_heatmap, _ = grad_cam.generate_cam(input_tensor)\n",
    "    \n",
    "    # Get class name\n",
    "    with open('imagenet_classes.txt') as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "    predicted_class_name = classes[predicted_class]\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.imshow(np.array(original_image))\n",
    "    plt.title(f'Original Image\\nPredicted: {predicted_class_name}')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # FEM results\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.imshow(fem_heatmap, cmap='jet')\n",
    "    plt.title('FEM Heatmap')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Grad-CAM results\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.imshow(grad_cam_heatmap, cmap='jet')\n",
    "    plt.title('Grad-CAM Heatmap')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Overlays\n",
    "    fem_overlay = 0.7 * np.array(original_image) + 0.3 * np.uint8(plt.cm.jet(fem_heatmap)[..., :3] * 255)\n",
    "    grad_cam_overlay = 0.7 * np.array(original_image) + 0.3 * np.uint8(plt.cm.jet(grad_cam_heatmap)[..., :3] * 255)\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(np.clip(fem_overlay, 0, 255).astype(np.uint8))\n",
    "    plt.title('FEM Overlay')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.imshow(np.clip(grad_cam_overlay, 0, 255).astype(np.uint8))\n",
    "    plt.title('Grad-CAM Overlay')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48d4ff6-366d-4cea-8eab-adbb7c653d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Path to your African Elephant image\n",
    "image_path = \"./subject4/LIME/data/African_elephant/ILSVRC2012_val_00048781.JPEG\"\n",
    "\n",
    "# Generate and visualize FEM\n",
    "visualize_fem(image_path, model)\n",
    "\n",
    "# Compare with Grad-CAM\n",
    "compare_fem_gradcam(image_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acc553-0b5e-4dc0-8c3e-685b7773b6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
