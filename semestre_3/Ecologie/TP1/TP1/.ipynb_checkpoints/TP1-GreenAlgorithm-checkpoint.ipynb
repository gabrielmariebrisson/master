{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYgSCQFfK50i"
   },
   "source": [
    "# From Hardware and infrastructure information to carbon emission when training a deep learning program\n",
    "\n",
    "---\n",
    "\n",
    "Master 2 Informatique - Université de Bordeaux\n",
    "Image et Son  - Image Processing and Computer Vision\n",
    "\n",
    "---\n",
    "Authors : Lucia Bouza, Aurélie Bugeau, Anne Vialard\n",
    "Acknowledgments : Anne-Laure Ligozat\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9SoVI2PIw_b"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This practical session explains how to measure the environmental impacts of deep learning code with the online tool: https://www.green-algorithms.org. The tool requires the entry of various data in order to measure the environmental impact of the executed code. This notebook will explain how to obtain this data.\n",
    "In this practice, you will measure a part of greenhouse gas emissions (GHG) related to energy consumption from the training of a digit classification network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhHBzahfwfHN"
   },
   "source": [
    "<span style=\"color:DarkRed\"> **You will run every command of the following practice both in google collab and on either a computer in cremi or your personal computer. You must answer to questions for both experiments.** </span>\n",
    "\n",
    "> Remark: If you choose your personal computer, you are in charge of all packages install.\n",
    "\n",
    "In Cremi, you must start by activating virtual environments in which all packages are installed with the following commands (uncomment them) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source  /net/ens/python/miniconda3/bin/activate\n",
    "#!source /net/ens/python/miniconda3/bin/activate codecarbon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch jupyter remotely:\n",
    "- log onto machine at cremi, via ssh. \n",
    "\n",
    "Then run \n",
    "``$jupyter notebook --no-browser --port 1235``\n",
    "\n",
    "\n",
    "- On your computer:\n",
    "``$ssh -N -L  1235:localhost:1235 -J jaguar.emi.u-bordeaux.fr user@machine``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEIxT5ezMMGA"
   },
   "source": [
    "## A - Collecting hardware infos\n",
    "\n",
    "This section will explain how to collect the necessary data for the use of the online tool Green Algorithms. Some of the necessary data is related to the hardware on which the deep learning program is trained. This part explains how to obtain this information depending on the operating system.\n",
    "\n",
    "> Remark: if you are running on Google Colab's, remember that notebook platform runs on Linux virtual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SOxeHcAE6ZA"
   },
   "source": [
    "With green algorithm, the energy consumption is computed as follows:\n",
    "$$C_{total} = run_{time} \\times \\rm{PUE} \\times \\rm{PSF} \\times \\left(P_{memory}+\\sum_{c\\in {cores}} P_{c}\\times \\rm{usage}_c\\right) \\tag{1}$$\n",
    "where\n",
    "- $run_{time}$ is the running time in hours\n",
    "- $\\rm{PUE}$ is the Power Usage Effectiveness. It is an efficiency coefficient of the data center.\n",
    "- $\\rm{PSF}$ is the Pragmatic Scaling Factor. This parameter is used to indicate how many times we have executed the code with the indicated configuration.\n",
    "- $P_{memory}$ is the power consumed by the memory in Watt.\n",
    "- $cores$ is the set of all CPU and GPU cores\n",
    "- $P_{c}$ is the power consumed by core $c$ (CPU or GPU) in Watt.\n",
    "- $\\rm{usage}_c$ is the usage factor of core $c$, between $0$ and $1$.\n",
    "\n",
    "\n",
    "The *carbon footprint* is calculated according to the following formula:\n",
    "$$CarbonFootprint = C_{total} \\times CI \\tag{2}$$\n",
    "where\n",
    "- $CI$ is the carbon intensity of the region where the code is being run\n",
    "\n",
    "In the following, we will explain how all these variables can be estimated. To do so, it is necessary to determine what platform we are running the notebook on. We can be running the code on our local computer, on an on-premises server, or in the cloud. Depending on the platform, we may be sharing resources with other processes, and there may also be other energy consuming elements involved. For example: storage, network devices, air conditioners, etc. If we are running the notebook on Colab, then we are running the code on GCP (Google Cloud Platform). If we are running it on our computer in Jupyter notebook then we are running it locally. There is also the possibility of running the notebook on a server of our educational institution or work. In this case, it is important to ask if the server is in a local datacenter or in the cloud.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU-gLXEZGO3R"
   },
   "source": [
    "### A.1 Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZNnR-_uE6ZC"
   },
   "source": [
    "> Remark: If you are working in Colab, you can choose to run the code with CPU, GPU or TPU by choosing the option in *Runtime > Change runtime type*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpd7487rMfS6"
   },
   "source": [
    "#### CPU\n",
    "\n",
    "To know the number of available CPUs and the model on the machine where you are running your program, you can execute the following commands. \n",
    "\n",
    "- **Linux**: `cat /proc/cpuinfo` will display information about the available CPUs. To make the information easy to read, we use the following script that determines the number of configured physical and logical CPUs. You can also use `lscpu`.\n",
    "- **MacOS**: `sysctl -n machdep.cpu.brand_string` will display Chip Brand, Processor Type, Chip Model and CPU Speed. Detailed info on cpu can further be obtained with the command `sysctl -a | grep machdep.cpu`. Other option can be browsing: **System Settings > General > About > Processor.**\n",
    "- **Windows**: `wmic cpu get name, numberofcores` will display same information as MacOS command. Other option can be browsing: **Task Manager > Performance > CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd8pfAS4E6ZD"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q1. Run the following command and observe the result</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l61oRb6M8roO",
    "outputId": "6fd708c5-b101-4327-db8d-9250ca38022d"
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo | \\\n",
    "awk -v FS=':' '                                       \\\n",
    "  /^physical id/ { if(nb_cpu<$2)  { nb_cpu=$2 } }     \\\n",
    "  /^cpu cores/   { if(nb_cores<$2){ nb_cores=$2 } }   \\\n",
    "  /^processor/   { if(nb_units<$2){ nb_units=$2 } }   \\\n",
    "  /^model name/  { model=$2 }                         \\\n",
    "                                                      \\\n",
    "  END{                                                \\\n",
    "   nb_cpu=(nb_cpu+1);                                 \\\n",
    "   nb_units=(nb_units+1);                             \\\n",
    "                                                      \\\n",
    "   print \"CPU model:\",model;                          \\\n",
    "   print nb_cpu,\"CPU,\",nb_cores,\"physical cores per CPU, total\",nb_units,\"logical CPU units\" \\\n",
    " }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s29_I9NOE-g6"
   },
   "source": [
    "> Remark: In virtual machines the information in the /proc/cpuinfo file may not be correct, and may represent some characteristics of the CPU emulated by the virtualizer. Unfortunately, from the virtual environment there is no way to know exactly the real CPU that is being used for the execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5le3dWPgTcdG"
   },
   "source": [
    "#### GPU\n",
    "\n",
    "Knowing the number and model of GPUs, can be done in the following ways:\n",
    "\n",
    "- **Linux**: `lshw -C display`\n",
    "- **MacOS**: browse: **System Settings > General > About > Graphics**\n",
    "- **Windows**: browse: **Task Manager > Performance > GPU**\n",
    "\n",
    "\n",
    "> Remark: The virtual machine used by Colab does not have some Linux commands. We know that the GPUs used by Colab are Nvidia, so we can use the nvidia-specific command:`!nvidia-smi -L`. This will show the number of available GPUs and their model and UUID. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkpS7CEFE6ZG"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q2. Run the following command and observe the result</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JT_vNH5QMk0b",
    "outputId": "cd688ca2-9b1d-4b17-8ef5-40c2bdbbbe6d"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bL17YlGw1N9B"
   },
   "source": [
    "#### Computing energy from cores\n",
    "\n",
    "To compute $P_c$ , Green Algorithms use the Thermal Design Power (**TDP**) of the model of the processing unit provided by the manufacturer. A core power usage is assumed to be equal to the TDP divided by the number of cores (if a chip has 2 cores and a TDP of 50W, then the TDP per core is 25).\n",
    "\n",
    "TDP is a specification that indicates the maximum amount of power that a computer processor (CPU or GPU) can dissipate when operating at its maximum performance. It refers to the power consumption under the maximum theoretical load.\n",
    "\n",
    "In general, CPUs with a higher number of cores will have a higher TDP because they require more power to operate at maximum performance. This is because each core in a CPU requires power to perform calculations, and the more cores a CPU has, the more power it will require. However, the relationship between TDP and the number of cores is not always straightforward. Some CPUs may have a lower TDP even though they have more cores, because they are designed to operate at a lower clock speed or have more efficient architecture. Similarly, some CPUs may have a higher TDP even though they have fewer cores, because they are designed to operate at a higher clock speed or have a less efficient architecture.\n",
    "\n",
    "If the **CPU or GPU model is not listed**,  Green Algorithms uses an average of 12 W per core.\n",
    "\n",
    "> Remark: Remember that for virtual environments, it is necessary to know the actual CPU, and the percentage of that CPU allocated to the virtual machine. In the event that this value cannot be obtained (as is the case in Colab), take an average value, but keep in mind that the data obtained will not be completely accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZF3c9L2vE6ZJ"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q3. Use http://calculator.green-algorithms.org/ or online documentations of CPU and GPU to find TDP values of your CPU and GPU, compute the value of $\\sum_{c\\in {cores}} P_{c}\\times \\rm{usage}_c$ for your  'personal' computer in Watt, considering $\\rm{usage}_c=1$ and $\\rm{PSF}=1$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sQmMPC-OY8O"
   },
   "source": [
    "### A.2 Memory\n",
    "\n",
    "According to  <a name=\"cite_ref-1\"></a>[[1]](#cite_note-1) GPUs are responsible for around 70% of power consumption, CPU for 15%, and RAM for 10%.\n",
    "Some tools like Green-Algorithms consider that power consumption of RAM depends strongly on the available memory, independently of the memory consumed.\n",
    "\n",
    "We can check the amount of available memory available for the GPU using the following commands:\n",
    "\n",
    "- **Linux**: `grep MemTotal /proc/meminfo`. The command returns the value in KB. It will be necessary to convert it to GB to use the tool (1 GB = 1024 MB = 1048576 KB). Another command is `free -h --si | awk  '/Mem:/{print $2}'`\n",
    "- **MacOS**: `system_profiler SPHardwareDataType | grep \"Memory:\"`. Other option  can be browsing: **System Settings > General > About > Memory.**\n",
    "- **Windows**: `systeminfo | findstr /C:\"Total Physical Memory\"`. The command returns the value in MB, it will be necessary to convert it to GB to use the tool. Other option  can be browsing: **Task Manager > Performance > Memory**\n",
    "\n",
    "\n",
    "Green Algorithms, considers a consumption of 0.3725 W per GB of memory available (if we have all the server memory available, it will account for all the server memory. If we are in an HPC cluster, it will account only for the amount of memory requested, regardless of how much the process consumes). The value 0.3725 was obtained experimentally <a name=\"cite_ref-2\"></a>[[2]](#cite_note-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4hLRkuhE6ZK"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q4. Run the following command and observe the result. Using Green Algorithm, verify the value of $P_{memory}$</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRsasV6IObdj",
    "outputId": "99ed8472-f687-446c-cf66-1c88293577e3"
   },
   "outputs": [],
   "source": [
    "!free -h --si | awk  '/Mem:/{print $2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrzmTx5PY3O3"
   },
   "source": [
    "### A.3 Run time  and usage factor $usage_c$\n",
    "\n",
    "The *run time* (or real time) refers to the duration of execution of the process, like using a stop watch.\n",
    "\n",
    "The *process time* is the amount of time during which a core (CPU, GPU or TPU) is used for processing instructions of a computer program. The total process time is the combination of the amount of time the cores spent performing some action for a program and the amount of time they spent performing system calls for the kernel on the program's behalf. <a name=\"cite_ref-3\"></a>[[3]](#cite_note-3).\n",
    "$$ process_{time} = user_{time} + system_{time} $$\n",
    "\n",
    "The cores usage factor is then the percentage of all available cores the job got, calculated as:\n",
    "$$ usage_{c} = \\frac{process_{time}}{(run_{time} * number_{cores})} $$\n",
    "\n",
    "We can measure the process time of the CPUs and the real time spent by the code with the `time` command (on Linux and MacOS). Unfortunately there is no similar command for Windows. It will be necessary to use language-specific libraries as `psutil` for Python. We will see an example below to use both methods.\n",
    "\n",
    "---\n",
    "Remark: If your CPU has more logical cores than physical cores, it uses hyperthreading: the hardware components of one physical core are shared between several threads. Each thread has at least its own set of registers. Most resources of the core (arithmetic and logic unit, floating point unit, cache) are shared between the threads. When hyperthreading is enabled, there are as many architectural state, including registers, per core as threads. These registers become visible to the operating system, which doubles the physical CPU capacity: for each core, one could theoretically reach 200% utilization according to standard monitoring tools, such as top. However, given the resources that are shared, this capacity cannot be achieved in practice, so in reality 200% is not the maximum real capacity. According to [intel]( https://www.intel.com/content/www/us/en/developer/articles/tool/performance-counter-monitor.html#cpu_utilization), the maximum capacity is up to 130%. (30% more than without hyperthreading). Measuring usage factor with hyperthreading is therefore not direct. \n",
    "\n",
    "*In CREMI, we will assume that \"number_cores' in previous formula is 30% more than the number of physical cores available.*\n",
    "\n",
    "*If you are running on you own computer with superuser privileges, you can run the following script to turn off hyper-threading (note that you might need to adapt the script for your environment)*\n",
    "\n",
    "<code>\n",
    "#!/bin/bash\n",
    "\n",
    "for cpunum in $(cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list | cut -s -d, -f2- | tr ',' '\\n' | sort -un)\n",
    "do\n",
    "    echo 0 > /sys/devices/system/cpu/cpu$cpunum/online\n",
    "done\n",
    "</code>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAm6F9ZcE6ZM"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q5. Give some cases where Real time may be greater than processing time, and vice versa.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX2Gh0bK8GYu"
   },
   "source": [
    "#### Measuring real time and CPU time when training a network\n",
    "\n",
    "To run experiments, you will use the code in `TrainingClassification.py`.\n",
    "\n",
    "<span style=\"color:DarkRed\">Q6. Briefly explain the architecture of the network.</span>\n",
    "\n",
    "After the model is defined, before moving on to training, a few more things need to be configured, which are added during the compile step:\n",
    "-    loss function: error function to minimize. We usually refer to it as loss or pérdida\n",
    "-    optimizer: algorithm used to optimize the function. Essentially, which gradient descent algorithm to use. It must be an object from the optimizers module, instantiated with the desired parameters, or a string indicating which one to use, in which case the default values are used.\n",
    "-    metrics: metrics to be used to monitor the evolution of the training.\n",
    "We use as optimizer an instance of SGD, with a learning rate lr=0.01, crossentropy loss and accuracy metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZEPoQK1_Sat"
   },
   "source": [
    "The training is done in 5 epochs and normally takes less than a minute on different infrastructures. This experiment runs on a single GPU if any.\n",
    "To get the process time used by the script, it is possible to use the command  `time`. The total time spent by the script corresponds with the real value (run time). The CPU time will be the sum of the user and sys values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time python TrainingClassification.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH7qPD0eE6ZO"
   },
   "source": [
    "It is also possible to use the libraries `psutil` and `time` to measure the time used by the script, and to obtain the process time. An example is provided in `TrainingClassification_withtime.py`. The total time spent by the script corresponds to the real value. The CPU time will be the sum of the user and sys values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python TrainingClassification_withtime.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xupyazgpE6ZQ"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q7. Do you observe some differences? <br> What is CPU approximate usage factors when running `TrainingClassification.py` ? You can use command htop while running to observe usage of logical cores. \n",
    "  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcGLRwZx7PfG"
   },
   "source": [
    "#### Measuring GPU time when training a network\n",
    "\n",
    "Unfortunately, there is no tool that can be used with the command line that gives us the total time of the script (whole time), the CPU time and the GPU time, in order to calculate the CPU and GPU usage factor required by Green-Algorithms.  \n",
    "\n",
    "\n",
    "\n",
    "> To measure the process time of GPUs, you may use the tool [NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) , when available. You need to add the time of all the child processes that are using the GPU. To make the task easier, we can send the data to a CSV file and perform the sum there.\n",
    "`!ncu --csv --metrics gpu__time_active  --target-processes all python TrainingClassification.py >> gpu.csv`\n",
    ">\n",
    ">[NVIDIA Nsight Compute](https://developer.nvidia.com/nsight-compute) can be used to measure GPU time, but requires the `libnvfuser_codgen.so` library, which is not present in Colab.\n",
    "\n",
    "Another option, used here, is to take empirical and specific measurements of the use of the GPU during the execution of their algorithm using the `nvidia-smi` tool, and extrapolate that value of GPU utilization to the entire execution. It is important to note that this utilization percentage corresponds to the total utilization, and not just the utilization of the process. There could be other processes running on the available GPUs. \n",
    "The python script `TrainingClassification_withtime.py` has a call to `nvidia-smi` in each epoch, leaving it's output in the file `nvidiaFile`.\n",
    "\n",
    "Up to our knowledge, there is also no tool that measures GPU time or GPU utilization for *non-Nvidia GPUs*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUrYVJ8rE6ZR"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q8. Observe the output file `nvidiaFile`. What is the mean  GPU utilization? How many processes are running on the GPU? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Practice with Green Algorithms\n",
    "\n",
    "<span style=\"color:DarkRed\">Q9. Using http://calculator.green-algorithms.org/, compute energy consumption for the program `TrainingClassification.py` when run in cremi/personal computer and Google Colab. For this question, consider you are running in France, with $PUE=1$ and $\\rm{PSF}=1$, and usage factors obtained previously. What do you think of the result?</span>\n",
    "\n",
    "<span style=\"color:DarkRed\">Q10. Use Green Algorithms to estimate energy consumption and carbon footprint of the following _scenario 1_, with $PUE=1$ and $\\rm{PSF}=1$. What do you think of the result? </span>\n",
    "|  Specifications |   | \n",
    "|---|---|\n",
    "| Execution time | 190h |   \n",
    "| Number of CPUs  |  4 |   \n",
    "| Type of CPUs  | Xeon E5-2683 v4  |  \n",
    "| Number of GPUs  | 4  | \n",
    "| Type of GPUs  |  Tesla V100 |  \n",
    "| Available memory |  32 Go |   \n",
    "| Location of the server | Orsay,France  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Other information on training and infrastructure information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFUpv87lSLBM"
   },
   "source": [
    "### C.1 Power Usage Efficiency (PUE)\n",
    "\n",
    "PUE is the efficiency coefficient of the data center. If PUE is not given, Green Algorithms considers the world average value given in 2019 for servers: 1.67, but we recommend considering the 2022 average value: 1.55 <a name=\"cite_ref-4\"></a>[[4]](#cite_note-4).\n",
    "\n",
    "For personal computers we generally consider PUE=1, as there are no other important devices consuming power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IddzZySE6ZS"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q11. Recall what is PUE.\n",
    "An important part of the non-IT consumption of a datacenter comes from air conditioning. What do you think about the relevance of using an average PUE?</span>\n",
    "\n",
    "In the following we will consider a PUE of 1 for personal computer, 1.1 for Google Collab and 1.2 for datacenter in Orsay. \n",
    "\n",
    "<span style=\"color:DarkRed\">Q12. What are the energy consumption obtained with Green Algorithms with these PUE values ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoT5mH19E6ZS"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q13. For Google datacenters, the PUE is 1.1 <a name=\"cite_ref-5\"></a>[[5]](#cite_note-5). Explain why this value is lower in Google  datacenters? Do you think this means every deep learning project should be run in Google datacenters? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image was taken from  <a name=\"cite_ref-6\"></a>[[6]](#cite_note-6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Guyon.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:DarkRed\">Q14. What is the value of PUE is this figure?</span>\n",
    "\n",
    "<span style=\"color:DarkRed\">Q15. Assuming that the data center's electricity consumption follows this distribution, what proportion of the electricity consumption of the IT room can be studied with Green Algorithms? Note that is this figure, only CPUs are considered, so it is not fully applicable to our case of study.</span>\n",
    "\n",
    "<span style=\"color:DarkRed\">Q16. Assuming CPU and GPU are part of the 43% in figure (c), what would be the complete energy consumption of the IT room when training digit classification code in Google Collab and _scenario 1_ ? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwPWpaj57KDX"
   },
   "source": [
    "### C.2 Pragmatic Scaling Factor\n",
    "\n",
    "This parameter is used to indicate how many times we have executed the code with the indicated configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKJzyqkDE6ZT"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q17. What is the pragmatic scaling factor until now for this practice?\n",
    "What is approximately the pragmatic scaling factor for the latest deep learning project you have conducting during your studies?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmrna4z1LihJ"
   },
   "source": [
    "### C.3 Location\n",
    "\n",
    "Carbon footprint is affected by the location from where the code is being executed. The origin of the energy used is key when determining greenhouse emissions. If we are running in the cloud, and the provider has several datacenters around the planet, it is sometimes possible to choose where you want to run the code from.\n",
    "\n",
    "We can check the execution location with the command:\n",
    "- **Linux, Windows and MacOS**: `curl ipinfo.io`\n",
    "\n",
    "> **Remark:** In the case of Colab, you cannot choose where to execute the codes. Nevertheless, you can also use previous command and then determine the datacenter where the code is executed with the following [link](https://cloud.google.com/about/locations?hl=es). We do not know in advance in which datacenter the virtual machine where the notebook will run will be created. It is necessary to execute the command every time we create or reset the runtime, since the location may change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiDOMMZwE6ZT"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q18. Run the following command and observe the result. What is the value of carbon intensity $CI$ used by Green Algorithm at the location where your code is running, according to [Green Algorithm](http://calculator.green-algorithms.org/)?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wj-pS9DbMhyn",
    "outputId": "7c3692df-f98b-4387-8ed1-234d427b5bdd"
   },
   "outputs": [],
   "source": [
    "!curl ipinfo.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remark: note that from this information on Google Colab, PUE could be updated with values taken from [5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zzIGyyVE6ZU"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q19. How do you think the emissions evolve with the country? Verify your hypotheses on Green Algorithm (for Sweden or Poland for instance). </span>\n",
    "\n",
    "\n",
    "Carbon intensity varies according to location but also to other variables, such as the time of day of execution, or the distribution of energy sources at a given moment.  It is important to mention that Green Algorithms does not yet take the information of carbon intensity in real time. The carbon intensity data is taken from [this file](https://github.com/GreenAlgorithms/green-algorithms-tool/blob/master/data/latest/CI_aggregated.csv), where for each country or region the source of information is specified. These values are an average.\n",
    "\n",
    "As additional information, carbon emissions of many countries can be checked in real time on the site https://app.electricitymaps.com/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfumoQgmE6ZU"
   },
   "source": [
    "<span style=\"color:DarkRed\">Q20. Compare the value of $CI$ you have just found with the one in real time from electrictymaps? What do you observe? Also compare with the last 24 hours and 30 days?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:DarkRed\">Q21.According to a Google research article  <a name=\"cite_ref-7\"></a>[[7]](#cite_note-7), Google's renewable energy purchases reduce the impact to zero (if running codes on Google servers). What do you think? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlyeARERE6ZY"
   },
   "source": [
    "## References\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>[1] [^](#cite_ref-1) M.Hodak,M.Gorkovenko,and A.Dholakia,“Towards power efficiency in deep learning on data center hardware,” 2019 IEEE International Conference on Big Data (Big Data), pp. 1814–1820, 2019.\n",
    "\n",
    "<a name=\"cite_note-2\"></a>[2] [^](#cite_ref-2) https://www.tomshardware.com/reviews/intel-core-i7-5960x-haswell-e-cpu,3918-13.html\n",
    "\n",
    "<a name=\"cite_note-3\"></a>[3] [^](#cite_ref-3) Wikipedia: https://en.wikipedia.org/wiki/Time_(Unix)\n",
    "\n",
    "<a name=\"cite_note-4\"></a>[4] [^](#cite_ref-4) Uptime Institute (https://uptimeinstitute.com/uptime_assets/6768eca6a75d792c8eeede827d76de0d0380dee6b5ced20fde45787dd3688bfe-2022-data-center-industry-survey-en.pdf)\n",
    "\n",
    "<a name=\"cite_note-5\"></a>[5] [^](#cite_ref-5) https://www.google.com/about/datacenters/efficiency/\n",
    "\n",
    "<a name=\"cite_note-6\"></a>[6] [^](#cite_ref-6) David Guyon. Supporting energy-awareness for cloud users. Networking and Internet Architecture, Université Rennes 1, 2018\n",
    "\n",
    "<a name=\"cite_note-7\"></a>[7] [^](#cite_ref-7)Patterson et al., The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink, arXiv:2204.05149, 2022 \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "full-user-track-cell": "",
  "full-user-track-date": "",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
