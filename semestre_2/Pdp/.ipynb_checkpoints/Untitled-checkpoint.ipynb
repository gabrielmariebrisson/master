{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e157927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e29b822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = GTZAN(root=\".\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c0212c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "root_dir = '/tmp/GTZAN'\n",
    "source_dir=os.getcwd()+'/Data_kaggle/images_original'\n",
    "\n",
    "if os.path.exists(root_dir):\n",
    "  shutil.rmtree(root_dir)\n",
    "\n",
    "def create_train_val_dirs(tmp_path,SOURCE_DIR):\n",
    "    \"\"\"\n",
    "    Creates directories for the train and test sets\n",
    "\n",
    "    Args:\n",
    "    tmp_path (string) - the base directory path to create subdirectories from\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE\n",
    "\n",
    "    # HINT:\n",
    "    # Use os.makedirs to create your directories with intermediate subdirectories\n",
    "    # Don't hardcode the paths. Use os.path.join to append the new directories to the tmp_path parameter\n",
    "\n",
    "\n",
    "    tmp_path_training = os.path.join(tmp_path, 'training')\n",
    "    tmp_path_validation = os.path.join(tmp_path, 'validation')\n",
    "    tmp_path_test = os.path.join(tmp_path, 'test')\n",
    "    \n",
    "    for repertory in os.listdir(SOURCE_DIR):\n",
    "        \n",
    "        repertory_path = os.path.join(SOURCE_DIR, repertory)\n",
    "        if not os.path.isdir(repertory_path) or repertory == \".DS_Store\":\n",
    "            continue\n",
    "            \n",
    "        tmp_path_training_repertory = os.path.join(tmp_path_training, repertory)\n",
    "        tmp_path_validation_repertory = os.path.join(tmp_path_validation, repertory)\n",
    "        tmp_path_test_repertory = os.path.join(tmp_path_test, repertory)\n",
    "\n",
    "        os.makedirs(tmp_path_training_repertory)\n",
    "        os.makedirs(tmp_path_validation_repertory)\n",
    "        os.makedirs(tmp_path_test_repertory)\n",
    "        \n",
    "try:\n",
    "    create_train_val_dirs(tmp_path=root_dir,SOURCE_DIR=source_dir )\n",
    "except FileExistsError:\n",
    "    print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "011efcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/GTZAN/test\n",
      "/tmp/GTZAN/training\n",
      "/tmp/GTZAN/validation\n",
      "/tmp/GTZAN/test/pop\n",
      "/tmp/GTZAN/test/metal\n",
      "/tmp/GTZAN/test/disco\n",
      "/tmp/GTZAN/test/blues\n",
      "/tmp/GTZAN/test/reggae\n",
      "/tmp/GTZAN/test/classical\n",
      "/tmp/GTZAN/test/rock\n",
      "/tmp/GTZAN/test/hiphop\n",
      "/tmp/GTZAN/test/country\n",
      "/tmp/GTZAN/test/jazz\n",
      "/tmp/GTZAN/training/pop\n",
      "/tmp/GTZAN/training/metal\n",
      "/tmp/GTZAN/training/disco\n",
      "/tmp/GTZAN/training/blues\n",
      "/tmp/GTZAN/training/reggae\n",
      "/tmp/GTZAN/training/classical\n",
      "/tmp/GTZAN/training/rock\n",
      "/tmp/GTZAN/training/hiphop\n",
      "/tmp/GTZAN/training/country\n",
      "/tmp/GTZAN/training/jazz\n",
      "/tmp/GTZAN/validation/pop\n",
      "/tmp/GTZAN/validation/metal\n",
      "/tmp/GTZAN/validation/disco\n",
      "/tmp/GTZAN/validation/blues\n",
      "/tmp/GTZAN/validation/reggae\n",
      "/tmp/GTZAN/validation/classical\n",
      "/tmp/GTZAN/validation/rock\n",
      "/tmp/GTZAN/validation/hiphop\n",
      "/tmp/GTZAN/validation/country\n",
      "/tmp/GTZAN/validation/jazz\n"
     ]
    }
   ],
   "source": [
    "# Test your create_train_val_dirs function\n",
    "\n",
    "for rootdir, dirs, files in os.walk(root_dir):\n",
    "    for subdir in dirs:\n",
    "        print(os.path.join(rootdir, subdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36aa7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=os.getcwd()+'/Data_kaggle/images_original'\n",
    "\n",
    "'''def convertSongToImage(path):\n",
    "    print(root)\n",
    "    for rootdir, dirs, files in os.walk(path):\n",
    "        for subdir in dirs:\n",
    "            print (label)\n",
    "            y,sr = librosa.load('myfile.wav')\n",
    "            df = pd.DataFrame(y, columns=['Amplitude'])'''\n",
    "\n",
    "def copy_data_from_source_to(tab_files, category, SOURCE_DIR, PATH_DIR):\n",
    "    for file in tab_files:\n",
    "        source_path = os.path.join(SOURCE_DIR, file)\n",
    "        destination_path = os.path.join(PATH_DIR, file)\n",
    "\n",
    "        if os.path.getsize(source_path) > 0:\n",
    "            shutil.copyfile(source_path, destination_path)\n",
    "        else:\n",
    "            print(f\"{file} is zero length, so ignoring.\")\n",
    "    print(tab_files[0])\n",
    "    #pixel_count = get_image_pixel_count(tab_files[0])\n",
    "    #print( tab_files[0].dtype)\n",
    "\n",
    "# SPLIT = 80/10/10\n",
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, TEST_DIR):\n",
    "    all_files = os.listdir(SOURCE_DIR)\n",
    "    random.shuffle(all_files)\n",
    "\n",
    "    split_one = int(len(all_files) * 0.8)\n",
    "    split_two = int(len(all_files) * 0.9)\n",
    "    \n",
    "    training_files = all_files[:split_one]\n",
    "    validation_files = all_files[split_one:split_two]\n",
    "    test_files = all_files[split_two:]\n",
    "\n",
    "    copy_data_from_source_to(training_files, \"pop\", SOURCE_DIR, TRAINING_DIR)\n",
    "    copy_data_from_source_to(validation_files, \"pop\", SOURCE_DIR, VALIDATION_DIR)\n",
    "    copy_data_from_source_to(test_files, \"pop\", SOURCE_DIR, TEST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5edf2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/GTZAN/test\n",
      "/tmp/GTZAN/training\n",
      "/tmp/GTZAN/validation\n",
      "pop00015.png\n",
      "pop00029.png\n",
      "pop00033.png\n",
      "Original directory has 100 images of  pop \n",
      "\n",
      "There are 80 images of  pop  for training\n",
      "There are 10 images of  pop  for validation\n",
      "There are 10 images of  pop  for testing\n",
      "metal00069.png\n",
      "metal00067.png\n",
      "metal00064.png\n",
      "Original directory has 100 images of  metal \n",
      "\n",
      "There are 80 images of  metal  for training\n",
      "There are 10 images of  metal  for validation\n",
      "There are 10 images of  metal  for testing\n",
      "disco00028.png\n",
      "disco00071.png\n",
      "disco00041.png\n",
      "Original directory has 100 images of  disco \n",
      "\n",
      "There are 80 images of  disco  for training\n",
      "There are 10 images of  disco  for validation\n",
      "There are 10 images of  disco  for testing\n",
      "blues00012.png\n",
      "blues00090.png\n",
      "blues00021.png\n",
      "Original directory has 100 images of  blues \n",
      "\n",
      "There are 80 images of  blues  for training\n",
      "There are 10 images of  blues  for validation\n",
      "There are 10 images of  blues  for testing\n",
      "reggae00036.png\n",
      "reggae00014.png\n",
      "reggae00004.png\n",
      "Original directory has 100 images of  reggae \n",
      "\n",
      "There are 80 images of  reggae  for training\n",
      "There are 10 images of  reggae  for validation\n",
      "There are 10 images of  reggae  for testing\n",
      "classical00071.png\n",
      "classical00045.png\n",
      "classical00091.png\n",
      "Original directory has 100 images of  classical \n",
      "\n",
      "There are 80 images of  classical  for training\n",
      "There are 10 images of  classical  for validation\n",
      "There are 10 images of  classical  for testing\n",
      "rock00041.png\n",
      "rock00081.png\n",
      "rock00017.png\n",
      "Original directory has 100 images of  rock \n",
      "\n",
      "There are 80 images of  rock  for training\n",
      "There are 10 images of  rock  for validation\n",
      "There are 10 images of  rock  for testing\n",
      "hiphop00035.png\n",
      "hiphop00076.png\n",
      "hiphop00019.png\n",
      "Original directory has 100 images of  hiphop \n",
      "\n",
      "There are 80 images of  hiphop  for training\n",
      "There are 10 images of  hiphop  for validation\n",
      "There are 10 images of  hiphop  for testing\n",
      "country00099.png\n",
      "country00082.png\n",
      "country00017.png\n",
      "Original directory has 100 images of  country \n",
      "\n",
      "There are 80 images of  country  for training\n",
      "There are 10 images of  country  for validation\n",
      "There are 10 images of  country  for testing\n",
      "jazz00026.png\n",
      "jazz00041.png\n",
      "jazz00033.png\n",
      "Original directory has 99 images of  jazz \n",
      "\n",
      "There are 79 images of  jazz  for training\n",
      "There are 10 images of  jazz  for validation\n",
      "There are 10 images of  jazz  for testing\n"
     ]
    }
   ],
   "source": [
    "TEST_DIR= os.path.join( root_dir,os.listdir(root_dir)[0]) \n",
    "TRAINING_DIR= os.path.join( root_dir,os.listdir(root_dir)[1])\n",
    "VALIDATION_DIR= os.path.join( root_dir,os.listdir(root_dir)[2])\n",
    "\n",
    "print (TEST_DIR)\n",
    "print (TRAINING_DIR)\n",
    "print (VALIDATION_DIR)\n",
    "\n",
    "for rootdir, dirs, files in os.walk(source_dir):\n",
    "    for subdir in dirs:\n",
    "        SUB_ROOT_DIR=os.path.join(rootdir, subdir)\n",
    "        \n",
    "        SUB_TEST_DIR=os.path.join(TEST_DIR, subdir)\n",
    "        SUB_TRAINING_DIR=os.path.join(TRAINING_DIR, subdir)\n",
    "        SUB_VALIDATION_DIR=os.path.join(VALIDATION_DIR, subdir)\n",
    "        \n",
    "        split_data(SOURCE_DIR=SUB_ROOT_DIR, TRAINING_DIR=SUB_TRAINING_DIR, VALIDATION_DIR=SUB_VALIDATION_DIR, TEST_DIR=SUB_TEST_DIR)\n",
    "        \n",
    "        print(f\"Original directory has {len(os.listdir(SUB_ROOT_DIR))} images of \",subdir,\"\\n\")\n",
    "        \n",
    "        print(f\"There are {len(os.listdir(SUB_TRAINING_DIR))} images of \",subdir,\" for training\")\n",
    "        print(f\"There are {len(os.listdir(SUB_VALIDATION_DIR))} images of \",subdir,\" for validation\")\n",
    "        print(f\"There are {len(os.listdir(SUB_TEST_DIR))} images of \",subdir,\" for testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14371a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR, TEST_DIR, img_size=(224, 224), batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates the training and validation data loaders.\n",
    "\n",
    "    Args:\n",
    "    TRAINING_DIR (string): Directory path containing the training images.\n",
    "    VALIDATION_DIR (string): Directory path containing the validation images.\n",
    "\n",
    "    Returns:\n",
    "    train_loader, validation_loader, test_loader - Tuple containing the loaders.\n",
    "    \"\"\"\n",
    "    # Define data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Create datasets\n",
    "    training_dataset = ImageFolder(root=TRAINING_DIR, transform=transform)\n",
    "    validation_dataset = ImageFolder(root=VALIDATION_DIR, transform=transform)\n",
    "    test_dataset = ImageFolder(root=TEST_DIR, transform=transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba449ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_generator, validation_generator, test_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR, TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08892065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d, ReLU, MaxPool2d, Linear\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomPoolingLayer(Module):\n",
    "    def forward(self, x):\n",
    "        # Calculate max, mean, and L2 norm along the specified dimension\n",
    "        max_values = F.adaptive_max_pool1d(x, 1)\n",
    "        mean_values = F.adaptive_avg_pool1d(x, 1)\n",
    "        l2_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n",
    "\n",
    "        # Concatenate the results along the feature dimension\n",
    "        result = torch.cat([max_values, mean_values, l2_norm], dim=1)\n",
    "\n",
    "        return result\n",
    "    \n",
    "\n",
    "class SimpleCNN(Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        # call the parent constructor\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=599,\n",
    "                kernel_size=(4, 128))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(4, 4), stride=(4, 4))\n",
    "        \n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=599, out_channels=149,\n",
    "            kernel_size=(4, 256))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        # initialize third set of CONV => RELU => POOL layers\n",
    "        self.conv3 = Conv2d(in_channels=149, out_channels=73,\n",
    "            kernel_size=(4, 256))\n",
    "        self.relu3 = ReLU()\n",
    "        self.maxpool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        \n",
    "        # initialize fourth set of CONV => RELU\n",
    "        self.conv4 = Conv2d(in_channels=73, out_channels=35,\n",
    "            kernel_size=(4, 512))\n",
    "        self.relu4 = ReLU()\n",
    "        \n",
    "        #adapte conv to dense\n",
    "        self.global_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # initialize custom pooling layer\n",
    "        self.custom_pooling = CustomPoolingLayer()\n",
    "        \n",
    "        # initialize first (and only) set of FC => RELU layers\n",
    "        self.relu5 = ReLU()\n",
    "        self.dense1 = nn.Linear(2048, 1)\n",
    "        \n",
    "        # initialize second (and only) set of FC => RELU layers\n",
    "        self.relu6 = ReLU()\n",
    "        self.dense2 = nn.Linear(2048, 1)\n",
    "        \n",
    "        # initialize our softmax classifier\n",
    "        self.relu7 = ReLU()\n",
    "        self.dense3 = nn.Linear(500, classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.maxpool3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        x = self.global_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = self.custom_pooling(x)\n",
    "        x = self.relu5(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu6(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        x = self.relu7(x)\n",
    "        \n",
    "        x = self.dense3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e5ce9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (71 x 76). Kernel size: (4 x 256). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_generator\u001b[38;5;241m.\u001b[39mdataset:\n\u001b[1;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     19\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 74\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[1;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n\u001b[0;32m---> 74\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(x)\n\u001b[1;32m     76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (71 x 76). Kernel size: (4 x 256). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an instance of the model\n",
    "numChannels = 3\n",
    "numClasses = 10 \n",
    "model = SimpleCNN(numChannels, numClasses)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train your model\n",
    "for epoch in range(15):\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_generator.dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print statistics after each epoch\n",
    "    average_loss = total_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    print(f'Epoch [{epoch + 1}/15], Loss: {average_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce27c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c2db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c29dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
