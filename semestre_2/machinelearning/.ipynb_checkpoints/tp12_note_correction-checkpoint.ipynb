{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f123c0",
   "metadata": {},
   "source": [
    "# 4TIN811U Machine Learning & Deep Learning\n",
    "# TP 12 (noté) - Effet de la *régularisation* sur le biais et la variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cca5e6-fc93-46de-827c-fb60b0b2dedd",
   "metadata": {},
   "source": [
    "# Commentaires sur la correction\n",
    "- Ceux qui ont utilisé `np.vander()` à la première question, sans aucun commentaire et sans le paramètre `increasing=True`, n'ont pas les degrés dans l'ordre demandé ; ça marche mais ça ne correspond pas à la consigne et ça montre une incompréhension de ce qui est fait (cela a été pénalisé faiblement). Aussi, cela implique que $\\theta_0$ est le dernier paramètre, ce qui rend le bonus plus compliqué à comprendre.\n",
    "- Pour le graphique de la norme, certains ont déduit erronément que la norme de $\\theta$ était $0$ jusqu'aux degrés élevés, car le graphique avait l'air plat au début. Toutefois, cela était dû à la croissance exponentielle de la norme, qui donnait l'illusion que les premières valeurs étaient nulles (car elles sont faibles comparativement aux dernières). Notez qu'un $\\theta$ nul n'aurait pas pu donner des polynômes intéressants au graphique précédent. Même sans utiliser une échelle logarithmique dans le graphique, vous pouviez juste `print` les valeurs de $||\\theta||$ pour constater que les valeurs n'étaient pas nulles.\n",
    "- Pour calculer le biais et la variance, presque personne n'a utilisé le code du cours 7 (duquel il fallait modifier 3 lignes, et dont le lien était rappelé au début du TP). J'y vois peu d'autres explications qu'un manque d'étude du contenu du cours. Beaucoup ont tenté des choses avec ChatGPT, qui se trompe régulièrement dans le sens sur lequel faire les moyennes (souvent, les moyennes sont faites sur les prédictions d'un même jeu de données, ce qui n'a pas de sens), mais tombait parfois sur les bonnes formules.\n",
    "- Pour le bonus, la question demandait d'afficher $\\theta_0$ après entraînement, ce qui était facilement faisable dès la première question mais que personne n'a fait (vous auriez vu que le premier élément du vecteur était toujours $0$, et cela rapportait déjà des points). ChatGPT n'a pas semblé comprendre la question (probablement car elle se trouvait loin du contexte, qui venait de la première question du TP).\n",
    "- Certains ont pu s'aider de modèles de langage (avec citation) et ont tout de même montré une compréhension et du recul par rapport à ce qui était généré. Toutefois, les phénomènes suivants, corrélés à l'utilisation de modèles de langage, ont été pénalisés :\n",
    "    - La présence d'informations superflues, non liées à la question. ChatGPT est plutôt loquace et en dit souvent trop, mais si vous copiez-collez ce qui est généré, il est nécessaire d'au moins montrer que vous comprenez quelles informations sont pertinentes pour répondre à la question. Certains ont écrit trois longs paragraphes pour une question sur 1 point, qui attendait une réponse d'une ou deux phrases.\n",
    "    - Certaines réponses, certes correctes, ignorent complètement ce qui a été fait précédemment dans le TP et recodent localement (de façon différente !) des fonctions réalisées aux questions précédentes. Cela montre un manque de compréhension des énoncés. De plus, ne pas factoriser son code est en général une mauvaise pratique.\n",
    "    - Il reste des traces évidentes de copiés-collés trop hâtifs : par exemple, les formules mathématiques (générées par des modèles de langage) n'ont souvent plus aucun sens après copié-collé. Cela n'a pas été pénalisé en cas de mention régulière d'une source dans le rendu, mais est sanctionné en l'absence de source (notez que \"source : Internet\" n'est pas une source suffisamment précise). Dans des travaux où vous disposez de plus de temps, si vous souhaitez copier-coller une réponse d'un modèle de langage, il est pertinent d'expliquer quel prompt vous avez utilisé, ou la méthodologie utilisée pour obtenir une telle réponse.\n",
    "    - Certaines questions demandaient explicitement de commenter le code, mais cela n'est souvent pas fait. Cela n'est pas un gros problème quand le code est clair et utilise des techniques basiques (ce qui est généralement le cas du code écrit par des humains), mais est plus sévèrement sanctionné en cas de code très concis utilisant des méthodes de numpy jamais utilisées en cours et dans les TPs : si vous utilisez des choses complexes, il faut montrer que vous les comprenez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b584e8-39d4-42da-bc2e-87948a563a51",
   "metadata": {},
   "source": [
    "# Énoncé\n",
    "***IMPORTANT :*** **Vous pouvez également partir de code déjà existant et le modifier, auquel cas il faut impérativement mentionner la source du code d'origine (même pour du code issu du cours ou des corrections des TPs).** Dans ce TP, vous **pouvez** utiliser `scikit-learn` quand cela est précisé. Le nombre de points de chaque question est noté avant la question.\n",
    "\n",
    "Vous pouvez bien sûr définir vos propres fonctions, rajouter des cases dans le notebook, ou rajouter des commentaires si vous le jugez utile. Si vous tombez à court de temps mais que tout ne fonctionne pas encore, n'hésitez pas à décrire où vous en êtes et ce qui, selon vous, pourrait être corrigé avec plus de temps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5ff8b-3a62-418c-9f48-82a0bafee6b7",
   "metadata": {},
   "source": [
    "Nous reprenons le code du cours 7 (**[qui se trouve sur Moodle](https://moodle.u-bordeaux.fr/mod/resource/view.php?id=414484&forceview=1)**). Nous allons approximer une courbe sinusoïdale avec des polynômes de degré plus ou moins élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a09c8a-4b94-4aca-b0b3-4c7bb59b1a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Settings\n",
    "n_train = 100  # Size of the training dataset\n",
    "n_test = 500 # Size of the test dataset\n",
    "noise = .1  # Standard deviation of the noise\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fonction à prédire\n",
    "def f(x):\n",
    "    return np.sin(10*x)\n",
    "\n",
    "xlim = [.1,.9]\n",
    "ylim = [-1.2,1.2]\n",
    "\n",
    "def generate(n_samples, noise):\n",
    "    X = np.random.rand(n_samples) * xlim[1]\n",
    "    X = np.sort(X)\n",
    "    Y = np.random.normal(f(X), noise, n_samples)\n",
    "    return X, Y\n",
    "\n",
    "X_ideal, Y_ideal = generate(n_samples=200, noise=0.0)\n",
    "X_train, Y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, Y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f027e-bfb4-43b0-ab8e-89ac4e9e6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot f\n",
    "plt.plot(X_ideal, Y_ideal, label=\"target function\")\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(X_train, Y_train, \".b\", label=\"samples: $y = f(x)+noise$\")\n",
    "\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.legend(loc=(1.1, 0.5))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5405c772-da16-4a8e-8bfa-77f10daedce8",
   "metadata": {},
   "source": [
    "**(/3)** Pour le moment, chaque $x$ dans `X_train` contient une valeur réelle.\n",
    "\n",
    "- À l'aide d'une **régression linéaire**, implémentez une fonction qui prend en paramètre un degré $k\\in\\mathbb{N}$ et retourne le polynôme de degré $k$\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x + \\dots + \\theta_kx^k\n",
    "$$\n",
    "qui minimise la fonction de coût \"moindres carrés\". *Utilisez `scikit-learn` pour appliquer la régression linéaire.*\n",
    "- **Expliquez dans votre code (avec un commentaire) comment vous augmentez le nombre de features de vos exemples (si vous utilisez une fonction de `numpy`, expliquez ce qu'elle représente).**\n",
    "- Vérifiez la taille de votre ensemble d'entraînement après avoir augmenté ses features en utilisant un `assert(condition)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b469b72-821e-4b6f-8fd8-4541288d2c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def augment_features(X, degree):\n",
    "    \"\"\"\n",
    "    Prend un paramètre un vecteur `X` contenant m exemples avec chacun une feature x, et retourne une matrice\n",
    "    avec m lignes telle que chaque ligne contient les features (1, x, x^2, ..., x^degree).\n",
    "    \"\"\"\n",
    "    # On prend toutes les puissances du vecteur X, qu'on considère comme un vecteur colonne, et on concatène ces colonnes horizontalement.\n",
    "    res = np.concatenate([np.power(X, k)[:,None] for k in range(degree + 1)], axis=1)\n",
    "    assert(res.shape == (X.shape[0], degree + 1)) # Il y a m exemples, chacun avec degree + 1 features (où degree = k).\n",
    "    return res\n",
    "\n",
    "# Code similaire à celui du cours 7.\n",
    "def fit_polynomial(X, Y, degree):\n",
    "    \"\"\"\n",
    "    Retourne le modèle scikit-learn qui calcule le polynôme de degré `degree`\n",
    "    qui minimise les moindres carrés sur les données `X` avec target `Y`.\n",
    "    \"\"\"\n",
    "    model = LinearRegression(fit_intercept=False) # Bonus : fit_intercept = False, sinon theta_0 = 0 car sklearn a son propre intercept term.\n",
    "    model.fit(augment_features(X, degree), Y)\n",
    "    return model\n",
    "\n",
    "augmented_features = augment_features(X_train, 3) # Devrait avoir taille (n_train, 4)\n",
    "print(augmented_features[:10]) # Matrice de largeur 4, telle que chaque ligne contient les puissances du nombre en position 1\n",
    "print(augmented_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2352d2-8652-4d12-9bea-9f668079e95b",
   "metadata": {},
   "source": [
    "On affiche les prédictions de votre modèle pour quelques valeurs de $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654a754-b3c6-428c-b51c-801300d54751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code du cours 7\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot f\n",
    "plt.plot(X_ideal, Y_ideal, label=\"target function\")\n",
    "\n",
    "# Plot training data (the first one)\n",
    "plt.plot(X_train, Y_train, \".b\", label=\"samples: $y = f(x)+noise$\")\n",
    "\n",
    "def apply_polynomial(model, X):\n",
    "    '''\n",
    "    Evaluates a linear regression model on an input sample\n",
    "    model: linear regression model\n",
    "    X: input sample\n",
    "    '''\n",
    "    degree = model.coef_.size - 1\n",
    "    Y = model.predict(augment_features(X, degree))\n",
    "    return Y\n",
    "\n",
    "def fit_plot(degree):\n",
    "    model = fit_polynomial(X_train, Y_train, degree)\n",
    "    print(model.coef_)\n",
    "    Y_predicted = apply_polynomial(model, X_train)\n",
    "    plt.plot(X_train, Y_predicted, label=\"polynomial of degree \" + str(degree))\n",
    "\n",
    "fit_plot(2)\n",
    "fit_plot(5)\n",
    "fit_plot(20)\n",
    "fit_plot(100)\n",
    "\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.legend(loc=(1.1, 0.5))\n",
    "\n",
    "plt.subplots_adjust(right=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096d1c7-0195-40ff-af63-cfe82ee3ba0a",
   "metadata": {},
   "source": [
    "**(/2)** Expliquez avec vos mots le dilemme entre *biais* et *variance*, et comment il est illustré par des prédictions utilisant des polynômes de degré varié.\n",
    "\n",
    "*Réponse :* L'erreur entre un modèle de prédiction et une fonction à prédire peut généralement être réduite à trois composantes :\n",
    "- Une composante appelée *erreur irréductible*, qui est due à du bruit (idéalement petit) dans les données. Cette composante est inévitable ; on ne peut a priori pas prédire le bruit sans avoir accès à davantage de features. Ici, l'erreur irréductible est représentée par la variable `noise`.\n",
    "- Une composante appelée *biais*, qui est la différence entre les prédictions et la fonction à prédire due à l'incapacité du modèle à représenter la fonction, généralement en raison d'une complexité trop faible du modèle. Un biais élevé est lié au phénomène de sous-apprentissage: le modèle n'est pas assez général pour prédire la fonction.\n",
    "- Une composante appelée *variance*, qui décrit la sensibilité du modèle appris à des petites variations de l'ensemble d'entraînement. Si un modèle prédit bien les données d'entraînement mais est très sensible, cela veut dire qu'il s'adapte trop à l'ensemble d'entraînement, ce qui entraîne du surapprentissage.\n",
    "\n",
    "Il y a généralement un compromis à trouver entre biais et variance, qui se traduit en un compromis sur la complexité du modèle à sélectionner (il faut un modèle complexe, mais pas trop).\n",
    "\n",
    "Pour les régressions polynomiales, augmenter le degré du polynôme donne des modèles plus généraux et plus complexes. Le biais diminue donc au fur et à mesure qu'on augmente le degré du polynôme. Par contre, au plus le degré du polynôme est élevé, au plus le modèle pourra s'adapter précisément aux données d'entraînement, donc la variance augmente. On peut voir sur le graphique ci-dessus qu'un polynôme de degré $2$ ne parvient pas à s'approcher de la fonction et a un biais élevé, alors qu'un polynôme de degré $100$ modèle trop bien les petites variations des données d'entraînement dues au bruit et semble avoir une variance élevée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045254f9-f91f-4f39-bbaa-e99de8d18b41",
   "metadata": {},
   "source": [
    "Dans la suite du TP, on va s'intéresser à la norme du vecteur $\\theta$, pour essayer d'établir un lien avec la question de biais et variance.\n",
    "\n",
    "**(/3)** Affichez un graphique qui donne la **norme euclidienne $||\\theta||$ de $\\theta$** en fonction du degré du polynôme (par exemple, pour tous les degrés de $1$ à $30$). Utilisez une échelle adaptée sur l'axe vertical pour que les valeurs soient plus lisibles et comparables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1311f8-b33b-4575-a53c-0679ff2f7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = []\n",
    "\n",
    "for degree in range(1, 31):\n",
    "    model = fit_polynomial(X_train, Y_train, degree)\n",
    "    # print(model.coef_) # Pour observer les valeurs de theta_0 pour le bonus\n",
    "    norm.append(np.linalg.norm(model.coef_))\n",
    "\n",
    "plt.yscale(\"log\") # Échelle logarithmique\n",
    "plt.xlabel(\"Degré\")\n",
    "plt.ylabel(\"$\\|\\\\theta\\|$\")\n",
    "plt.plot(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe394f-a936-452a-929f-62969c9b1f03",
   "metadata": {},
   "source": [
    "Que constatez-vous sur la magnitude du vecteur $\\theta$ ?\n",
    "\n",
    "*Réponse :* Les valeurs de norme de $\\theta$ augmentent en fonction du degré de deviennent rapidement très grandes ($> 10^{10}$ pour des grands degrés) comparées aux données qui sont toutes entre $0$ et $1$. Pour mieux visualiser cet effet, on peut utiliser une échelle logarithmique pour les valeurs de $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14911d3-c78c-4b53-b650-250900c57cb4",
   "metadata": {},
   "source": [
    "Pour limiter l'effet de ce que vous avez observé à la question précédente, on peut modifier la fonction de coût \"moindres carrés\" de la façon suivante. Pour un hyperparamètre $\\alpha \\ge 0$ fixé, on définit une fonction de coût\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{2}\\left(\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\\right) + \\alpha ||\\theta||^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653893f8-dadc-4788-836e-9209e931cde8",
   "metadata": {},
   "source": [
    "**(/1)** Expliquez la différence dans cette formule par rapport à la régression linéaire classique, et l'effet que cela a sur les vecteurs $\\theta$ appris.\n",
    "\n",
    "*Réponse :* La différence est le terme supplémentaire $\\alpha||\\theta||^2$. On note que quand $\\alpha = 0$, on retrouve exactement la fonction de coût classique pour la régression linéaire. Quand $\\alpha$ est non-nul, le terme supplémentaire induit une \"pénalité\" ; au plus la norme de $\\theta$ est grande, au plus le coût est élevé. La fonction de coût essaie donc toujours de minimiser les moindres carrés, mais en limitant la norme du vecteur $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733bacc-2456-4aca-bb50-25f847800489",
   "metadata": {},
   "source": [
    "**(/2)** Cette régression linéaire modifiée correspond à la *ridge regression*, qui est une [fonction disponible dans `scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). En utilisant `scikit-learn`, montrez l'effet que vous avez discuté à la réponse ci-dessus sur un graphique adapté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b7dc54-0207-4201-b402-3e6a0919d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def fit_polynomial_regularized(X, Y, degree, alpha):\n",
    "    \"\"\"\n",
    "    Retourne le modèle scikit-learn qui calcule le polynôme de degré `degree`\n",
    "    qui minimise la fonction de coût J sur les données `X` avec target `Y`.\n",
    "    \"\"\"\n",
    "    model = Ridge(fit_intercept=False, alpha=alpha) # Bonus : fit_intercept = False, sinon theta_0 = 0 car sklearn a son propre intercept term.\n",
    "    model.fit(augment_features(X, degree), Y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ef74c-ea2e-452f-96c8-7ed93b371c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_to_test = [0.1, 1, 2, 5, 10, 50, 100, 1000, 10000] # Vous pouvez tester les alphas dans ce tableau\n",
    "\n",
    "def get_norm_ridge(X, Y, degree, alpha):\n",
    "    model = fit_polynomial_regularized(X, Y, degree, alpha)\n",
    "    assert(np.size(model.coef_) == degree + 1)\n",
    "    return np.linalg.norm(model.coef_)\n",
    "\n",
    "for alpha in alphas_to_test:\n",
    "    norms = [get_norm_ridge(X_train, Y_train, i, alpha) for i in range(1,31)]\n",
    "    plt.plot(range(1,31), norms, label=f\"alpha = {alpha}\")\n",
    "\n",
    "plt.legend(loc=(1.1, 0.5))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('$\\|\\\\theta\\|$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b155cb9-e5e2-471d-af85-91f777acb607",
   "metadata": {},
   "source": [
    "On peut voir qu'au plus $\\alpha$ est grand, au moins la norme du $\\theta$ prédit est grande."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b04fd-f505-49eb-b8dd-ff4354c5405c",
   "metadata": {},
   "source": [
    "**(/3)** Rappelez (en français) comment vous pouvez approximer le biais et la variance en générant plusieurs datasets avec du bruit.\n",
    "\n",
    "*Réponse :* Cela est discuté à la fin du cours 7 sur Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef3600-db8f-4267-bdc9-060b968cfc93",
   "metadata": {},
   "source": [
    "Implémentez une fonction qui retourne le biais et la variance (expérimentale) d'un modèle de *ridge regression* pour un degré et un $\\alpha \\ge 0$ en argument, et calculez quelques exemples de valeurs. Vous pouvez utiliser la fonction `generate(n_samples, noise)` définie plus haut pour générer plusieurs datasets avec du bruit. **Veillez à commenter votre code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5abf45-f3b9-4a17-be3d-f75de17630e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "n_repeat = 100 # Nombre d'expériences considérées pour faire des statistiques\n",
    "\n",
    "# Code inspiré de celui du cours 7.\n",
    "def bias_variance(degree, alpha):\n",
    "    \"\"\" Retourne le biais et la variance empiriques d'un modèle qui calcule un modèle\n",
    "    polynomial de degré `degree` en utilisant une ridge regression avec paramètre `alpha`.\"\"\"\n",
    "    X_train_repeated = np.zeros((n_train, n_repeat))\n",
    "    Y_train_repeated = np.zeros((n_train, n_repeat))\n",
    "\n",
    "    # On génère n_repeat datasets avec du bruit\n",
    "    for i in range(n_repeat):\n",
    "        X, Y = generate(n_samples=n_train, noise=noise)\n",
    "        X_train_repeated[:,i] = X\n",
    "        Y_train_repeated[:,i] = Y\n",
    "    \n",
    "    Y_predicted = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    # On entraîne le modèle sur les données d'entraînement, et on calcule les prédictions sur les données de test \n",
    "    for i in range(n_repeat):\n",
    "        model = fit_polynomial_regularized(X_train_repeated[:,i], Y_train_repeated[:,i], degree, alpha)\n",
    "        Y_predicted[:,i] = apply_polynomial(model, X_test)\n",
    "\n",
    "    # On calcule le biais : on fait la moyenne de la différence entre la moyenne des prédictions et la valeur \"réelle\"\n",
    "    avg_y = np.mean(Y_predicted, axis = 1)\n",
    "    bias_2 = np.linalg.norm(avg_y - f(X_test)) / n_test\n",
    "\n",
    "    # On calcule la variance : on fait la moyenne des différences entre la moyenne des prédictions et les valeurs prédites.\n",
    "    variance = 0.0\n",
    "    for i in range(n_repeat):\n",
    "        variance += np.linalg.norm(avg_y - Y_predicted[:,i])\n",
    "    variance /= n_test * n_repeat\n",
    "\n",
    "    return bias_2, variance\n",
    "\n",
    "print(bias_variance(5, 0.1))\n",
    "print(bias_variance(5, 1))\n",
    "print(bias_variance(20, 1))\n",
    "print(bias_variance(20, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d60aefe-744b-443e-a5fa-578c7a9fab67",
   "metadata": {},
   "source": [
    "**(/6)** Discutez et étudiez l'impact de l'hyperparamètre $\\alpha \\ge 0$ sur le biais et la variance. Vous pouvez argumenter :\n",
    "- sur base d'éléments formels (en réfléchissant à l'intuition via les formules et les définitions), et/ou\n",
    "- sur base d'éléments empiriques (par exemple, en récoltant des statistiques sur de nombreuses valeurs et en affichant un graphique soutenant votre argumentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0135ec-f00e-4f7f-8004-1b1b25fd19c3",
   "metadata": {},
   "source": [
    "*Réponse :*\n",
    "Nous allons argumenter que **$\\alpha$ augmente le biais et diminue la variance**.\n",
    "Formellement, il est attendu que des valeurs plus grandes de $\\alpha$ augmentent le biais : en effet, le terme additionnel dans la fonction de coût, qui est indépendant de $x$ et $y$, biaise le calcul de la minimisation des moindres carrés. Les prédictions ont donc tendance à être moins bonnes.\n",
    "Il est aussi attendu que des valeurs plus grandes de $\\alpha$ diminuent la variance : quand $\\alpha$ est grand, des prédictions différentes pour des datasets différents ont tendance à être plus proches, car les vecteurs $\\theta$ sont plus petits.\n",
    "\n",
    "Observons ce phénomène sur des graphiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30560cca-e3cb-4bc4-b907-8209e9a7dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "degree_max = 20\n",
    "\n",
    "alphas_to_test = [0.1, 1, 2, 5, 10, 50, 100, 1000, 10000]\n",
    "\n",
    "x_to_plot = []\n",
    "y_to_plot = []\n",
    "\n",
    "fig, axs = plt.subplots(degree_max, 2, figsize=(15, 100))\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlabel('$\\\\alpha$')\n",
    "    ax.set_xscale('log') # Échelle logarithmique pour l'axe horizontal\n",
    "\n",
    "for degree in tqdm(range(1, degree_max + 1)):\n",
    "    biases = []\n",
    "    variances = []\n",
    "    for alpha in alphas_to_test:\n",
    "        bias, variance = bias_variance(degree, alpha)\n",
    "        biases.append(bias)\n",
    "        variances.append(variance)\n",
    "\n",
    "    axs[degree - 1, 0].plot(alphas_to_test, biases)\n",
    "    axs[degree - 1, 0].set_ylabel(\"biais\")\n",
    "    axs[degree - 1, 0].set_title(f\"degree = {degree}\")\n",
    "    axs[degree - 1, 1].plot(alphas_to_test, variances)\n",
    "    axs[degree - 1, 1].set_ylabel(\"variance\")\n",
    "    axs[degree - 1, 1].set_title(f\"degree = {degree}\")\n",
    "\n",
    "fig.suptitle('Biais sur la gauche, variances sur la droite')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.97)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63ac8df-e969-4a88-8dcc-272af244dd2b",
   "metadata": {},
   "source": [
    "**Bonus (/1,5)** Quand vous affichez les valeurs du vecteur $\\theta$ après apprentissage (pour la régression linéaire et pour la *ridge regression* avec `scikit-learn`), observez-vous quelque chose de surprenant sur la valeur du coefficient $\\theta_0$ ?\n",
    "\n",
    "*Réponse :* $\\theta_0 = 0$ dans tous les cas ! Cela est dû au fait que `scikit-learn` a déjà alloué le terme de biais (correspondant à la feature $1 = x^0$ et appelé *intercept_term* dans la documentation), et que $\\theta_0$ n'est donc pas réellement utilisé. \n",
    "\n",
    "En vous basant sur la [documentation de la régression linéaire de `scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html), expliquez à quoi cela peut être dû. Modifiez une ligne de votre code plus haut pour éviter cela (ajoutez `# Bonus` en commentaire à côté de la ligne)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
