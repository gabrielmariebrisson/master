{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f123c0",
   "metadata": {},
   "source": [
    "# 4TIN811U Machine Learning & Deep Learning\n",
    "# TP 2 - Régression locale et régression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f8bf92-292a-49c8-9261-f35c641dcb89",
   "metadata": {},
   "source": [
    "## Commentaires généraux sur vos soumissions\n",
    "- Merci de rendre uniquement le fichier `.ipynb`, sans le mettre dans une archive compressée.\n",
    "- Si vous observez que vous obtenez des résultats incohérents mais que vous n'avez pas pu trouver l'erreur dans votre code, nous apprécions quand vous ajoutez un commentaire expliquant que vous savez que les résultats sont étranges, et que vous émettez une ou deux hypothèses sur ce qui pourrait être corrigé avec plus de temps. Par exemple, certains obtiennent une prédiction des prix qui croît en la distance au centre-ville ; essayez davantage d'interpréter et d'analyser vos résultats.\n",
    "- Globalement, on préfère recevoir un TP incomplet mais qui fonctionne bien, plutôt que toutes les questions complétées mais sans que rien ne fonctionne. Par exemple, il n'est pas pertinent d'attaquer la régression locale à plusieurs dimensions si le cas à une dimension ne peut déjà pas s'exécuter sans erreur.\n",
    "\n",
    "## Commentaires sur le TP2\n",
    "- Pour la fonction de poids `weight`, notez qu'en général, la distance entre deux vecteurs est une norme et pas une valeur absolue. Pour beaucoup, cela limitait votre fonction `weight` à des points à une seule dimension.\n",
    "- Pour ceux qui ont des lignes dans tous les sens sur les graphiques du poids, modifiez le paramètre \"marker\" de la fonction plot (ici, le paramètre \"o\" convenait bien).\n",
    "- Une légère optimisation est possible pour certains qui avaient bien une fonction weight autorisant des vecteurs, au moment de calculer la norme au carré : pour plus d'efficacité et pour diminuer les erreurs numériques, ne pas faire `np.linalg.norm(x) ** 2` car cela passe par un calcul inutile (et plutôt coûteux) de racine carrée ; `np.inner(x, x)` donne le résultat voulu directement, sans calcul de racine carrée.\n",
    "- Pour la multiplication matricielle, préférez `numpy.matmul()` à `np.dot()`. Le fonction `np.dot()` effectue également une multiplication matricielle quand on lui passe des matrices en entrée, mais a d'autres comportements quand on lui passe d'autres objets en entrée ; pour diminuer les risques d'erreurs, `numpy.matmul()` retourne une erreur quand les objets en entrée ne sont pas des matrices. \n",
    "- Pour la régression locale, évitez les calculs matriciels redondants ; ici, la multiplication \"X.T @ W\" était présente deux fois dans la formule, mais vous pouvez ne la calculer qu'une seule fois.\n",
    "- Dans la méthode de Newton, évitez de calculer plusieurs fois f(x) inutilement (l'évaluation de certaines fonctions peut être coûteuse).\n",
    "- Pour les deux racines de `f` : beaucoup ont trouvé \"-1.000000000001768\" et \"-1\" ; ces deux valeurs correspondent à la même racine (-1), la différence est juste due à l'erreur numérique et au epsilon choisi ! Le graphique suggère qu'il y a bien deux racines distinctes. Il fallait essayer ici quelques valeurs de départ différentes pour trouver les deux racines.\n",
    "- Essayez de bien comprendre la différence entre les fonctions passées en paramètre de la méthode de Newton en fonction de si on cherche une racine ou un maximum/minimum.\n",
    "- Pour la méthode de Newton multi-dimensionnelle : elle prend en entrée des fonctions qui ne dépendent que d'un paramètre (ici, theta) mais la fonction `gradient_l` (resp. `hessian_l`) prend trois (resp. deux) paramètres en entrée. Ici, X et Y étaient fixés ; une façon propre de contourner ce problème est de définir une fonction lambda avec un paramètre utilisant les valeurs fixées de X et Y. Par exemple pour le gradient : `gradient_l_theta = lambda theta: gradient_l(X, Y, theta)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e7b0214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette option peut être décommentée pour améliorer la visualisation\n",
    "# des figures, mais ne fonctionne pas sur tous les systèmes.\n",
    "#%matplotlib widget\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562c8fd",
   "metadata": {},
   "source": [
    "# 1. Régression locale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6a3ed6",
   "metadata": {},
   "source": [
    "Dans cette première partie, nous allons prédire [le prix des logements à Perth](https://www.kaggle.com/datasets/syuzai/perth-house-prices) en utilisant une régression locale à partir de deux informations : leur surface et leur distance au centre-ville.\n",
    "Le dataset <code>dataset_tp2_houses.npy</code> est disponible dans le fichier <code>tp2.zip</code> sur Moodle et contient 1000 données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de798bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = np.load(\"dataset_tp2_houses.npy\") # 1000 données, trois colonnes : surface, distance du centre-ville, prix\n",
    "print(np.shape(house_prices))\n",
    "print(house_prices)\n",
    "\n",
    "surfaces = house_prices[:,0]\n",
    "distances = house_prices[:,1]\n",
    "prices = house_prices[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc118f0",
   "metadata": {},
   "source": [
    "Affichez le graphique en 3D du prix des logements en fonction de leur surface et de leur distance au centre-ville."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8f5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.scatter3D(surfaces, distances, prices)\n",
    "ax.set_xlabel(\"Distance from city centre\")\n",
    "ax.set_ylabel(\"Distance from city centre\")\n",
    "ax.set_zlabel(\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1965c8d",
   "metadata": {},
   "source": [
    "Implémentez la fonction de poids $w^{(i)}$ vue au cours, qui donne un poids à un élément $x^{(i)}$ des données d'entraînement en fonction d'un $x$ fixé et de l'hyperparamètre de bande passante $\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight(x, xi, tau):\n",
    "    \"\"\"Returns the weight to give xi from the training set\n",
    "       with respect to a point x and with bandwidth tau.\"\"\"\n",
    "    diff = np.subtract(xi, x)\n",
    "    return np.exp(- np.inner(diff, diff) / (2 * (tau ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6bbf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_vector(x, X, tau):\n",
    "    \"\"\"Returns the vector of weights for each element of vector X\n",
    "       with respect to a point x and with bandwidth tau.\"\"\"\n",
    "    return np.array([weight(x, xi, tau) for xi in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea846708",
   "metadata": {},
   "source": [
    "Pour un x fixé (par exemple, 20000), affichez dans un graphique les poids des différents éléments de la feature \"distance au centre-ville\". Essayez différentes valeurs de bande passante. Qu'observez-vous ? Qu'en est-il si vous passez à la feature \"surface\" ?\n",
    "\n",
    "*Réponse :* Au plus $\\tau$ est grand, au plus la courbe est \"large\", c'est-à-dire donne des poids non-négligeables à des points plus éloignés de $x$. Quand on passe à la feature \"surface\", on observe que les courbes sont beaucoup plus larges pour un même $\\tau$. Cela est dû à la magnitude moins grande des données, qui ne varient qu'entre ~50 et ~600. Les différences sont donc beaucoup moins grandes en moyenne et, dans la formule, $\\tau$ doit contrebalancer des numérateurs beaucoup moins grands. Un même $\\tau$ n'est donc pas pertinent pour les deux features simultanément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique pour la feature \"distance au centre-ville\"\n",
    "plt.figure()\n",
    "x = 20000\n",
    "plt.plot(distances, weight_vector(x, distances, 1), \"o\", label=\"tau=1\")\n",
    "plt.plot(distances, weight_vector(x, distances, 100), \"o\", label=\"tau=100\")\n",
    "plt.plot(distances, weight_vector(x, distances, 3000), \"o\", label=\"tau=3000\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Distance from city centre\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a3bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique pour la feature \"surface\"\n",
    "plt.figure()\n",
    "x = 200\n",
    "plt.plot(surfaces, weight_vector(x, surfaces, 1), \"o\", label=\"tau=1\")\n",
    "plt.plot(surfaces, weight_vector(x, surfaces, 50), \"o\", label=\"tau=50\")\n",
    "plt.plot(surfaces, weight_vector(x, surfaces, 400), \"o\", label=\"tau=400\")\n",
    "plt.xlabel(\"Surfaces\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14264700",
   "metadata": {},
   "source": [
    "### 1.1. Régression locale à une caractéristique\n",
    "\n",
    "Dans cette première partie, on souhaite estimer le prix d'un logement à Perth en prenant *uniquement en compte la distance au centre-ville* (pour simplifier les calculs, on ignore pour le moment la surface). Effectuez un pré-traitement pour récupérer le $X$ et le $Y$ adaptés à partir de la variable <code>house_prices</code>. N'oubliez pas de prendre en compte le paramètre $\\theta_0$ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = house_prices.shape\n",
    "X = house_prices[:, 1:2]\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "Y = house_prices[:, n-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05a13f",
   "metadata": {},
   "source": [
    "Pour chaque $x$, calculez le vecteur $\\theta$ minimisant la fonction d'erreur $J'_x(\\theta)$ vue au cours pour la régression locale. Utilisez votre méthode favorite parmi celles vues au cours (ou utilisez les deux et comparez les résultats !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On utilise ici la méthode exacte.\n",
    "def theta_minimizing_error(X, Y, x, tau, weight_function=weight_vector):\n",
    "    m, _ = X.shape\n",
    "\n",
    "    # Construction de la matrice W\n",
    "    weights = weight_function(x, X, tau)\n",
    "    W = np.zeros((m, m))\n",
    "    for i in range(m):\n",
    "        W[i][i] = .5 * weights[i]\n",
    "    \n",
    "    # Précalcul pour éviter une multiplication matricielle redondante\n",
    "    XTW = X.T @ W\n",
    "\n",
    "    # Formule exacte vue au cours\n",
    "    return np.linalg.inv(XTW @ X) @ XTW @ Y\n",
    "\n",
    "def prediction(X, Y, x, tau, weight_function=weight_vector):\n",
    "    one_x = np.concatenate(([1], np.atleast_1d(x)))\n",
    "    theta = theta_minimizing_error(X, Y, one_x, tau, weight_function=weight_function)\n",
    "    return one_x.T @ theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c3d0a",
   "metadata": {},
   "source": [
    "Affichez les prédictions données par votre fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_points = 300\n",
    "to_evaluate = np.linspace(np.min(X[:,1]), np.max(X[:,1]), nb_points)\n",
    "plt.figure()\n",
    "plt.plot(distances, Y, 'o')\n",
    "plt.xlabel(\"Distance from city centre\")\n",
    "plt.ylabel(\"Price\")\n",
    "\n",
    "tau = 3000\n",
    "predictions = [prediction(X, Y, x, tau) for x in to_evaluate]\n",
    "plt.plot(to_evaluate, predictions, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aab7860",
   "metadata": {},
   "source": [
    "### 1.2. Régression locale à plusieurs caractéristiques\n",
    "\n",
    "En utilisant les mêmes fonctions, faites fonctionner votre méthode en prenant cette fois en compte les deux features \"surface\" et \"distance du centre-ville\". Affichez vos résultats sur un graphique en trois dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = house_prices.shape\n",
    "X = house_prices[:, 0:2]\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "Y = house_prices[:, n-1]\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.scatter3D(X[:,1], X[:,2], Y)\n",
    "ax.set_xlabel(\"Surface\")\n",
    "ax.set_ylabel(\"Distance from city centre\")\n",
    "ax.set_zlabel(\"Price\")\n",
    "\n",
    "t0 = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 30)\n",
    "t1 = np.linspace(np.min(X[:,2]), np.max(X[:,2]), 30)\n",
    "to_evaluate = np.array([[x0, x1] for x0 in t0 for x1 in t1])\n",
    "tau = 3000\n",
    "predictions = [prediction(X, Y, x, tau) for x in to_evaluate]\n",
    "ax.scatter3D(to_evaluate[:,0], to_evaluate[:,1], predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857739c",
   "metadata": {},
   "source": [
    "En fonction de votre choix de bande passante $\\tau$, il est probable que selon une des deux dimensions, les prédictions soient linéaires. À quoi cela peut-il être dû ?\n",
    "\n",
    "*Réponse :* Cela est dû au fait que, comme discuté plus haut, un même $\\tau$ n'est pas pertinent pour les deux dimensions à la fois. Ici, le $\\tau$ choisi (3000) donne une bonne fonction de poids pour la feature \"Distance from city center\", mais les variations de surface n'ont presque aucun impact sur le poids. Cela implique que selon la dimension \"surface\", les prédictions sont linéaires car quand les poids sont proches de $1$, la régression locale ressemble à une régression linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a8c81",
   "metadata": {},
   "source": [
    "Pour remédier à cela, il peut être utile de *normaliser* les données.\n",
    "Ré-appliquez vos fonctions en vous assurant que les éléments de $X$ ont la même magnitude selon chaque dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9511a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "m, n = house_prices.shape\n",
    "X = np.concatenate((house_prices[:, 0:1], house_prices[:, 1:2]), axis=1)\n",
    "Y = house_prices[:, n-1]\n",
    "\n",
    "# Normalisation utilisant StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.scatter3D(X[:,1], X[:,2], Y)\n",
    "ax.set_xlabel(\"Surface\")\n",
    "ax.set_ylabel(\"Distance from city centre\")\n",
    "ax.set_zlabel(\"Price\")\n",
    "\n",
    "t0 = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 30)\n",
    "t1 = np.linspace(np.min(X[:,2]), np.max(X[:,2]), 30)\n",
    "to_evaluate = np.array([[x0, x1] for x0 in t0 for x1 in t1])\n",
    "tau = 1\n",
    "predictions = [prediction(X, Y, x, tau) for x in to_evaluate]\n",
    "ax.scatter3D(to_evaluate[:,0], to_evaluate[:,1], predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d560039",
   "metadata": {},
   "source": [
    "Généralisez vos fonctions afin de pouvoir facilement changer la fonction de poids utilisée.\n",
    "Utilisez la fonction de poids $w^{(i)} = 1$ et commentez le résultat.\n",
    "\n",
    "*Réponse :* En appliquant une fonction de poids constante, tous les données ont le même poids, indépendamment de la distance au point évalué. On retombe en fait exactement sur la régression linéaire classique (comme au TP1), qui est généralisée par la régression locale quand les poids ne sont pas constants. Les prédictions sont donc linéaires et forment un plan de l'espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcaac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def identity_weight(x, X, tau):\n",
    "    return [1 for _ in X]\n",
    "\n",
    "m, n = house_prices.shape\n",
    "X = np.concatenate((house_prices[:, 0:1], house_prices[:, 1:2]), axis=1)\n",
    "Y = house_prices[:, n-1]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "ax.scatter3D(X[:,1], X[:,2], Y)\n",
    "ax.set_xlabel(\"Surface\")\n",
    "ax.set_ylabel(\"Distance from city centre\")\n",
    "ax.set_zlabel(\"Price\")\n",
    "\n",
    "t0 = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 30)\n",
    "t1 = np.linspace(np.min(X[:,2]), np.max(X[:,2]), 30)\n",
    "to_evaluate = np.array([[x0, x1] for x0 in t0 for x1 in t1])\n",
    "tau = 1\n",
    "predictions = [prediction(X, Y, x, tau, weight_function=identity_weight) for x in to_evaluate]\n",
    "ax.scatter3D(to_evaluate[:,0], to_evaluate[:,1], predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ca1a1",
   "metadata": {},
   "source": [
    "## 2. Méthode de Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4cfbfb",
   "metadata": {},
   "source": [
    "On considère la fonction $$f(x) = x^4 + 3x^3 - 6x^2 + 4x + 12.$$\n",
    "\n",
    "En utilisant la méthode de Newton (à une dimension), calculez\n",
    "* ses racines,\n",
    "* son minimum global.\n",
    "\n",
    "Utilisez la même fonction <code>newton_1d</code> (avec des arguments différents) pour les deux tâches données.\n",
    "Aidez-vous du graphe de $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**4 + 3 * x**3 - 6 * x**2 + 4 * x + 12\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"Computes the derivative of f at point x.\"\"\"\n",
    "    return 4 * x**3 + 9 * x**2 - 12 * x + 4\n",
    "\n",
    "def ddf(x):\n",
    "    \"\"\"Computes the second derivative of f at point x.\"\"\"\n",
    "    return 12 * x**2 + 18 * x - 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2829ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphe de f entre -5 et 5\n",
    "plt.figure()\n",
    "values = np.linspace(-5, 5, 10000)\n",
    "plt.plot(values, [f(x) for x in values], \"-\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21feb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_1d(f, df, x0, max_iter=1000, epsilon=1e-10):\n",
    "    \"\"\"Finds a root of f given its derivative df and an initial guess x0.\n",
    "       Assumes that a value x is a root if abs(f(x)) < epsilon.\n",
    "       Raises an exception if no such value is found within max_iter iterations.\"\"\"\n",
    "    fx0 = f(x0)\n",
    "    i = 0\n",
    "    while abs(fx0) > epsilon and i < max_iter:\n",
    "        x0 = x0 - fx0 / df(x0)\n",
    "        fx0 = f(x0)\n",
    "        i += 1\n",
    "    if abs(fx0) > epsilon and i >= max_iter:\n",
    "        raise Exception(\"No root of f found in newton_1d within the given number of iterations.\")\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa670352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the two roots and the global minimum using newton_1d\n",
    "# Les valeurs de départ donnant les deux racines ont été obtenues après quelques tests de valeurs initiales différentes\n",
    "r1 = newton_1d(f, df, 3)\n",
    "r2 = newton_1d(f, df, 0)\n",
    "print(\"Racines : \", r1, r2)\n",
    "\n",
    "minimum = newton_1d(df, ddf, 3)\n",
    "print(\"Minimum : \", minimum)\n",
    "\n",
    "plt.figure()\n",
    "values = np.linspace(-5, 5, 10000)\n",
    "plt.plot(values, [f(x) for x in values], \"-\")\n",
    "plt.plot([r1, r2, minimum], [f(r1), f(r2), f(minimum)], \"o\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914bc9c",
   "metadata": {},
   "source": [
    "## 3. Régression logistique\n",
    "Dans cette partie, on souhaite prédire [la qualité de pommes](https://www.kaggle.com/datasets/nelgiriyewithana/apple-quality/data) en utilisant une régression logistique à partir de deux informations : leur taille et leur *sweetness*. Le deux features sont des réels (déjà normalisés) et la qualité est un entier dans $\\{0, 1\\}$ : $1$ pour les bonnes pommes et $0$ pour les mauvaises pommes.\n",
    "Le dataset <code>dataset_tp2_apples.npy</code> est disponible dans le fichier <code>tp2.zip</code> sur Moodle et contient 1000 données d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d61c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 données, trois colonnes : taille, sweetness et qualité\n",
    "apple_quality = np.load(\"dataset_tp2_apples.npy\", allow_pickle=True)\n",
    "print(np.shape(apple_quality))\n",
    "print(apple_quality)\n",
    "\n",
    "size = apple_quality[:,0]\n",
    "sweetness = apple_quality[:,1]\n",
    "quality = apple_quality[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e3cab",
   "metadata": {},
   "source": [
    "Affichez un graphique en 2D (une dimension par feature) reprenant les différentes données, en utilisant des symboles différents pour les bonnes et les mauvaises pommes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40809896",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "good_idx = np.where(quality == 1)\n",
    "bad_idx = np.where(quality == 0)\n",
    "plt.plot(size[good_idx], sweetness[good_idx], \"o\", label=\"good\")\n",
    "plt.plot(size[bad_idx], sweetness[bad_idx], \"^\", label=\"bad\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Sweetness\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19c1da",
   "metadata": {},
   "source": [
    "Nous avons vu au cours que la fonction de log-vraisemblance pour la régression logistique est la suivante:\n",
    "$$\\ell(\\theta) = \\sum_{i = 1}^m y^{(i)}\\log h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)})),$$\n",
    "où $h_\\theta(x^{(i)}) = \\frac{1}{1 + e^{-\\theta^{T}x^{(i)}}}$.\n",
    "\n",
    "Nous avons aussi vu que les dérivées partielles de $\\ell$ sont\n",
    "$$\\frac{\\partial}{\\partial\\theta_j}\\ell(\\theta) = \\sum_{i=1}^m (y^{(i)} - h_\\theta(x^{(i)}))x_j^{(i)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534056e6",
   "metadata": {},
   "source": [
    "On peut donc calculer les entrées de la matrice hessienne: pour $0 \\le j, k \\le n$, l'élément en position $(j, k)$ est donné par\n",
    "$$\n",
    "\\begin{align}\n",
    "H_{jk}(\\theta) &= \\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k} \\ell(\\theta) \\\\\n",
    "       &= \\sum_{i = 1}^m -\\frac{\\partial}{\\partial\\theta_j}h_\\theta(x^{(i)})x^{(i)}_k \\\\\n",
    "       &= -\\sum_{i = 1}^m h_\\theta(x^{(i)})(1 - h_\\theta(x^{(i)}))x^{(i)}_j x^{(i)}_k.\n",
    "\\end{align}\n",
    "$$\n",
    "(Assurez-vous de pouvoir suivre les calculs !)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad178c3a",
   "metadata": {},
   "source": [
    "*Remarque* : Nous avons déclaré au cours (sans preuve) que $\\ell$ est concave, et a donc un seul maximum local, qui est de plus global. Cela peut en fait se prouver en analysant les propriétés de la matrice hessienne: celle-ci est *semi-définie négative* pour tout $\\theta$, c'est-à-dire, pour tout $z\\in\\mathbb{R}^{n+1}$, $z^TH(\\theta)z \\le 0$. Cela peut se vérifier (exercice !) avec la définition ci-dessus.\n",
    "Cette propriété est la généralisation multi-dimensionnelle de la propriété \"pour $f\\colon \\mathbb{R} \\to \\mathbb{R}$, $f$ est concave si $f'' < 0$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005388d7",
   "metadata": {},
   "source": [
    "Implémentez des fonctions qui calculent $h_\\theta$, le gradient de $\\ell$ et la matrice hessienne de $\\ell$. Notez que la matrice hessienne ne dépend plus de $Y$ !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ec5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(xi, theta):\n",
    "    return 1 / (1 + np.exp(-theta.T @ xi))\n",
    "\n",
    "def gradient_l(X, Y, theta):\n",
    "    m, n = X.shape\n",
    "    commonFactor = np.zeros((m))\n",
    "    for i in range(0, m):\n",
    "        commonFactor[i] = Y[i] - h(X[i], theta)\n",
    "    gradient = np.zeros((n))\n",
    "    for j in range(0, n):\n",
    "        for i in range(0, m):\n",
    "            gradient[j] += commonFactor[i] * X[i][j]\n",
    "    return gradient\n",
    "\n",
    "def hessian_l(X, theta):\n",
    "    m, n = X.shape\n",
    "    commonFactor = np.zeros((m))\n",
    "    for i in range(0, m):\n",
    "        hxi = h(X[i], theta)\n",
    "        commonFactor[i] = hxi * (1 - hxi)\n",
    "    H = np.zeros((n, n))\n",
    "    for j in range(0, n):\n",
    "        for k in range(0, n):\n",
    "            for i in range(0, m):\n",
    "                H[j][k] -= commonFactor[i] * X[i][j] * X[i][k]\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82532e4e",
   "metadata": {},
   "source": [
    "Utilisez la matrice hessienne et la méthode de Newton (multi-dimensionnelle cette fois) pour trouver les valeurs $\\theta = (\\theta_0, \\theta_1, \\theta_2)$ maximisant la vraisemblance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5847fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_multidim(f, df, theta_0, max_iter=1000, epsilon=1e-6):\n",
    "    \"\"\"Finds a root of multivariate function f given its gradient df and an initial guess theta_0.\n",
    "       Assumes that a value theta is a root if np.linalg.norm(f(theta)) < epsilon.\n",
    "       Raises an exception if no such value is found within max_iter iterations.\"\"\"\n",
    "    f_theta_0 = f(theta_0)\n",
    "    i = 0\n",
    "    while np.linalg.norm(f_theta_0) > epsilon and i < max_iter:\n",
    "        theta_0 = theta_0 - np.linalg.inv(df(theta_0)) @ f_theta_0\n",
    "        f_theta_0 = f(theta_0)\n",
    "        i += 1\n",
    "    if np.linalg.norm(f_theta_0) > epsilon and i >= max_iter:\n",
    "        raise Exception(\"No root of f found in newton_multidim within the given number of iterations.\")\n",
    "    return theta_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d253fce",
   "metadata": {},
   "source": [
    "Ajoutez à votre graphique précédent la frontière de décision (qui est la droite telle que $\\theta^Tx = 0$ pour le vecteur $\\theta$ à trouver, qui correspond à la droite des points $x$ donnant la valeur $\\frac{1}{2}$ à la fonction sigmoïde) donnée par la régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = apple_quality[:, 0:2]\n",
    "m, n = X.shape\n",
    "X = np.concatenate((np.ones((m, 1)), X), axis=1)\n",
    "Y = apple_quality[:, 2]\n",
    "\n",
    "gradient_l_theta = lambda theta: gradient_l(X, Y, theta)\n",
    "H_theta = lambda theta: hessian_l(X, theta)\n",
    "theta_max = newton_multidim(gradient_l_theta, H_theta, np.zeros((n+1)))\n",
    "print(theta_max)\n",
    "\n",
    "plt.figure()\n",
    "good_idx = np.where(quality == 1)\n",
    "bad_idx = np.where(quality == 0)\n",
    "plt.plot(size[good_idx], sweetness[good_idx], \"o\", label=\"good\")\n",
    "plt.plot(size[bad_idx], sweetness[bad_idx], \"^\", label=\"bad\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Sweetness\")\n",
    "\n",
    "to_evaluate = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 100)\n",
    "plt.plot(to_evaluate, [(-theta_max[0] - theta_max[1] * x) / theta_max[2] for x in to_evaluate], \"-\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
